{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a981605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: /GPU:0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_15816/149252551.py\", line 563, in train_step  *\n        p11_pred = predict_uniaxial_p11(uniaxial_l1, model)\n    File \"/tmp/ipykernel_15816/149252551.py\", line 464, in predict_uniaxial_p11  *\n        p11_raw = _calculate_raw_uniaxial_p11(lambda1, model)\n    File \"/tmp/ipykernel_15816/149252551.py\", line 452, in _calculate_raw_uniaxial_p11  *\n        I1, I2, I4, I6 = get_invariants_tf(l1_t, l2_t, l3_t)\n    File \"/tmp/ipykernel_15816/149252551.py\", line 428, in get_invariants_tf  *\n        tf.debugging.assert_greater(lambda1, 0.0, message=\"λ1 must be > 0\")\n\n    TypeError: Input 'y' of 'Greater' Op has type float32 that does not match type float64 of argument 'x'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 591\u001b[39m\n\u001b[32m    589\u001b[39m epochs = \u001b[32m300\u001b[39m  \u001b[38;5;66;03m# retained\u001b[39;00m\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m     total_loss, loss1, loss2, reg = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43muniaxial_l1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniaxial_p11\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiaxial_l2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiaxial_p22\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_weight_biaxial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    592\u001b[39m     loss_history.append(\u001b[38;5;28mfloat\u001b[39m(total_loss.numpy()))\n\u001b[32m    593\u001b[39m     current_lr = lr_schedule(optimizer.iterations)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/Chicken-Disease-Classification/chicken/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_filet9gxf0nk.py:11\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[39m\u001b[34m(uniaxial_l1, uniaxial_p11, biaxial_l2, biaxial_p22, loss_weight)\u001b[39m\n\u001b[32m      9\u001b[39m retval_ = ag__.UndefinedReturnValue()\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ag__.ld(tf).GradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     p11_pred = \u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_uniaxial_p11\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43muniaxial_l1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     rel_err_1 = (ag__.ld(p11_pred)[\u001b[32m1\u001b[39m:] - ag__.ld(uniaxial_p11)[\u001b[32m1\u001b[39m:]) / (ag__.converted_call(ag__.ld(tf).abs, (ag__.ld(uniaxial_p11)[\u001b[32m1\u001b[39m:],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) + ag__.ld(REL_DEN_EPS))\n\u001b[32m     13\u001b[39m     loss1 = ag__.converted_call(ag__.ld(tf).reduce_mean, (ag__.converted_call(ag__.ld(huber), (ag__.converted_call(ag__.ld(tf).zeros_like, (ag__.ld(rel_err_1),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), ag__.ld(rel_err_1)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_fileqtvo3zyl.py:11\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_uniaxial_p11\u001b[39m\u001b[34m(lambda1, model)\u001b[39m\n\u001b[32m      9\u001b[39m do_return = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     10\u001b[39m retval_ = ag__.UndefinedReturnValue()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m p11_raw = \u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_calculate_raw_uniaxial_p11\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m p11_offset = ag__.converted_call(ag__.ld(_calculate_raw_uniaxial_p11), (ag__.converted_call(ag__.ld(tf).constant, ([\u001b[32m1.0\u001b[39m],), \u001b[38;5;28mdict\u001b[39m(dtype=ag__.ld(tf).float64), fscope), ag__.ld(model)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[32m0\u001b[39m]\n\u001b[32m     13\u001b[39m p11 = ag__.ld(p11_raw) - ag__.ld(p11_offset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_filehui5jc63.py:16\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf___calculate_raw_uniaxial_p11\u001b[39m\u001b[34m(l1, model)\u001b[39m\n\u001b[32m     14\u001b[39m     l3_t = ag__.converted_call(ag__.ld(tf).pow, (ag__.ld(l1_t), -\u001b[32m0.5\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     15\u001b[39m     ag__.converted_call(ag__.ld(tape).watch, ([ag__.ld(l1_t), ag__.ld(l2_t)],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     I1, I2, I4, I6 = \u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_invariants_tf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml1_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml2_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml3_t\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     W = ag__.converted_call(ag__.ld(model), (ag__.ld(I1), ag__.ld(I2), ag__.ld(I4), ag__.ld(I6)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     18\u001b[39m dWdl1 = ag__.converted_call(ag__.ld(tape).gradient, (ag__.ld(W), ag__.ld(l1_t)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_file4wk2geqt.py:11\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf__get_invariants_tf\u001b[39m\u001b[34m(lambda1, lambda2, lambda3)\u001b[39m\n\u001b[32m      9\u001b[39m retval_ = ag__.UndefinedReturnValue()\n\u001b[32m     10\u001b[39m min_lambda_val = ag__.converted_call(ag__.ld(tf).constant, (\u001b[32m1e-06\u001b[39m,), \u001b[38;5;28mdict\u001b[39m(dtype=ag__.ld(tf).float64), fscope)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebugging\u001b[49m\u001b[43m.\u001b[49m\u001b[43massert_greater\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mλ1 must be > 0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m ag__.converted_call(ag__.ld(tf).debugging.assert_greater, (ag__.ld(lambda2), \u001b[32m0.0\u001b[39m), \u001b[38;5;28mdict\u001b[39m(message=\u001b[33m'\u001b[39m\u001b[33mλ2 must be > 0\u001b[39m\u001b[33m'\u001b[39m), fscope)\n\u001b[32m     13\u001b[39m ag__.converted_call(ag__.ld(tf).debugging.assert_greater, (ag__.ld(lambda3), \u001b[32m0.0\u001b[39m), \u001b[38;5;28mdict\u001b[39m(message=\u001b[33m'\u001b[39m\u001b[33mλ3 must be > 0\u001b[39m\u001b[33m'\u001b[39m), fscope)\n",
      "\u001b[31mTypeError\u001b[39m: in user code:\n\n    File \"/tmp/ipykernel_15816/149252551.py\", line 563, in train_step  *\n        p11_pred = predict_uniaxial_p11(uniaxial_l1, model)\n    File \"/tmp/ipykernel_15816/149252551.py\", line 464, in predict_uniaxial_p11  *\n        p11_raw = _calculate_raw_uniaxial_p11(lambda1, model)\n    File \"/tmp/ipykernel_15816/149252551.py\", line 452, in _calculate_raw_uniaxial_p11  *\n        I1, I2, I4, I6 = get_invariants_tf(l1_t, l2_t, l3_t)\n    File \"/tmp/ipykernel_15816/149252551.py\", line 428, in get_invariants_tf  *\n        tf.debugging.assert_greater(lambda1, 0.0, message=\"λ1 must be > 0\")\n\n    TypeError: Input 'y' of 'Greater' Op has type float32 that does not match type float64 of argument 'x'.\n"
     ]
    }
   ],
   "source": [
    "# --- Dependencies ---\n",
    "# If running in Colab or a fresh env, keep these.\n",
    "# !pip install tensorflow -q\n",
    "# !pip install -U \"seaborn\" --quiet\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Reproducibility ---\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# --- Setup, Model Definition, Helper/Physics Functions ---\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "DEVICE = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Global numeric guards\n",
    "ARG_CLIP_MIN = tf.constant(-10.0, dtype=tf.float64)\n",
    "ARG_CLIP_MAX = tf.constant(10.0, dtype=tf.float64)\n",
    "EPS = tf.constant(1e-12, dtype=tf.float64)          # general epsilon\n",
    "REL_DEN_EPS = tf.constant(1e-8, dtype=tf.float64)   # epsilon for relative error denom\n",
    "NEAR_ZERO_TOL = tf.constant(1e-10, dtype=tf.float64)\n",
    "\n",
    "# Caps for learned exponents to prevent overflow in pow(I, k) and pow(arg, i)\n",
    "K_MIN = tf.constant(1.0, dtype=tf.float64)\n",
    "K_MAX = tf.constant(6.0, dtype=tf.float64)     # keep moderate growth on invariants\n",
    "I_MIN = tf.constant(1.0, dtype=tf.float64)\n",
    "I_MAX = tf.constant(4.0, dtype=tf.float64)     # cap i-exponents\n",
    "# Optional: toggle invariant normalization (kept OFF by default to preserve your physics)\n",
    "NORMALIZE_INVARIANTS = False\n",
    "\n",
    "@tf.function\n",
    "def smooth_relu(x, beta=20.0):\n",
    "    \"\"\"A smooth, differentiable approximation of ReLU to avoid kinks near 0.\"\"\"\n",
    "    return tf.nn.softplus(beta * x) / beta\n",
    "\n",
    "class StrainEnergyANN_Layered_TF(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"StrainEnergyModel\")\n",
    "        # All model parameters are defined here. b1 is computed, not trainable.\n",
    "\n",
    "        # ---- k's ----\n",
    "        self.raw_log_k1 = self.add_weight(\n",
    "            name=\"raw_log_k1\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k2 = self.add_weight(\n",
    "            name=\"raw_log_k2\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k3 = self.add_weight(\n",
    "            name=\"raw_log_k3\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k4 = self.add_weight(\n",
    "            name=\"raw_log_k4\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k5 = self.add_weight(\n",
    "            name=\"raw_log_k5\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k6 = self.add_weight(\n",
    "            name=\"raw_log_k6\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k7 = self.add_weight(\n",
    "            name=\"raw_log_k7\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k8 = self.add_weight(\n",
    "            name=\"raw_log_k8\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k9 = self.add_weight(\n",
    "            name=\"raw_log_k9\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k10 = self.add_weight(\n",
    "            name=\"raw_log_k10\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k11 = self.add_weight(\n",
    "            name=\"raw_log_k11\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k12 = self.add_weight(\n",
    "            name=\"raw_log_k12\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k13 = self.add_weight(\n",
    "            name=\"raw_log_k13\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k14 = self.add_weight(\n",
    "            name=\"raw_log_k14\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k15 = self.add_weight(\n",
    "            name=\"raw_log_k15\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_k16 = self.add_weight(\n",
    "            name=\"raw_log_k16\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        # ---- i's ----\n",
    "        self.raw_log_i1 = self.add_weight(\n",
    "            name=\"raw_log_i1\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_i2 = self.add_weight(\n",
    "            name=\"raw_log_i2\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_i3 = self.add_weight(\n",
    "            name=\"raw_log_i3\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_i4 = self.add_weight(\n",
    "            name=\"raw_log_i4\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_i5 = self.add_weight(\n",
    "            name=\"raw_log_i5\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_i6 = self.add_weight(\n",
    "            name=\"raw_log_i6\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_i7 = self.add_weight(\n",
    "            name=\"raw_log_i7\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_i8 = self.add_weight(\n",
    "            name=\"raw_log_i8\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        # ---- a's ----\n",
    "        self.raw_log_a1 = self.add_weight(\n",
    "            name=\"raw_log_a1\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_a2 = self.add_weight(\n",
    "            name=\"raw_log_a2\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_a3 = self.add_weight(\n",
    "            name=\"raw_log_a3\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_a4 = self.add_weight(\n",
    "            name=\"raw_log_a4\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_a5 = self.add_weight(\n",
    "            name=\"raw_log_a5\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_a6 = self.add_weight(\n",
    "            name=\"raw_log_a6\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_a7 = self.add_weight(\n",
    "            name=\"raw_log_a7\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_a8 = self.add_weight(\n",
    "            name=\"raw_log_a8\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        # ---- b's ----\n",
    "        self.raw_log_b2 = self.add_weight(\n",
    "            name=\"raw_log_b2\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.01)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_b3 = self.add_weight(\n",
    "            name=\"raw_log_b3\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_b4 = self.add_weight(\n",
    "            name=\"raw_log_b4\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_b5 = self.add_weight(\n",
    "            name=\"raw_log_b5\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_b6 = self.add_weight(\n",
    "            name=\"raw_log_b6\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_b7 = self.add_weight(\n",
    "            name=\"raw_log_b7\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_b8 = self.add_weight(\n",
    "            name=\"raw_log_b8\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.1)),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        # ---- primes ----\n",
    "        self.raw_log_a3_prime = self.add_weight(\n",
    "            name=\"raw_log_a3_prime\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.2)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_a4_prime = self.add_weight(\n",
    "            name=\"raw_log_a4_prime\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.2)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_a5_prime = self.add_weight(\n",
    "            name=\"raw_log_a5_prime\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.2)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_a6_prime = self.add_weight(\n",
    "            name=\"raw_log_a6_prime\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.2)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_a7_prime = self.add_weight(\n",
    "            name=\"raw_log_a7_prime\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.2)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_a8_prime = self.add_weight(\n",
    "            name=\"raw_log_a8_prime\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.2)),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.raw_log_b3_prime = self.add_weight(\n",
    "            name=\"raw_log_b3_prime\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.2)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_b4_prime = self.add_weight(\n",
    "            name=\"raw_log_b4_prime\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.2)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_b5_prime = self.add_weight(\n",
    "            name=\"raw_log_b5_prime\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.2)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_b6_prime = self.add_weight(\n",
    "            name=\"raw_log_b6_prime\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.2)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_b7_prime = self.add_weight(\n",
    "            name=\"raw_log_b7_prime\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.2)),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.raw_log_b8_prime = self.add_weight(\n",
    "            name=\"raw_log_b8_prime\", shape=(), dtype=tf.float64,\n",
    "            initializer=tf.keras.initializers.Constant(value=tf.math.log(0.2)),\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        # constants\n",
    "        self.three = tf.constant(3.0, dtype=tf.float64)\n",
    "        self.one = tf.constant(1.0, dtype=tf.float64)\n",
    "        self.pow_base_epsilon = tf.constant(1e-8, dtype=tf.float64)\n",
    "\n",
    "\n",
    "    # --- term helpers (unchanged physics, with existing argument clipping) ---\n",
    "    def _term_power_law(self, I, k, i, c, ref_val):\n",
    "        arg = smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k))\n",
    "        return c * tf.pow(arg + self.pow_base_epsilon, i)\n",
    "\n",
    "    def _term_exponential(self, I, k, i, ic, oc, ref_val):\n",
    "        arg = smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k))\n",
    "        return oc * (tf.exp(tf.clip_by_value(ic * tf.pow(arg + self.pow_base_epsilon, i), ARG_CLIP_MIN, ARG_CLIP_MAX)) - 1.0)\n",
    "\n",
    "    def _term_cosh_minus_one_with_i(self, I, k, i, ic, oc, ref_val):\n",
    "        arg = smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k))\n",
    "        return oc * (tf.cosh(tf.clip_by_value(ic * tf.pow(arg + self.pow_base_epsilon, i), ARG_CLIP_MIN, ARG_CLIP_MAX)) - 1.0)\n",
    "\n",
    "    def _term_sinh_with_i(self, I, k, i, ic, oc, ref_val):\n",
    "        arg = smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k))\n",
    "        return oc * tf.sinh(tf.clip_by_value(ic * tf.pow(arg + self.pow_base_epsilon, i), ARG_CLIP_MIN, ARG_CLIP_MAX))\n",
    "\n",
    "    def _term_identity_scaled(self, I, k, c, ref_val):\n",
    "        return c * (tf.pow(I, k) - tf.pow(ref_val, k))\n",
    "\n",
    "    def _term_exponential_no_i(self, I, k, ic, oc, ref_val):\n",
    "        return oc * (tf.exp(tf.clip_by_value(ic * (tf.pow(I, k) - tf.pow(ref_val, k)), ARG_CLIP_MIN, ARG_CLIP_MAX)) - 1.0)\n",
    "\n",
    "    def _term_cosh_minus_one(self, I, k, ic, oc, ref_val):\n",
    "        return oc * (tf.cosh(tf.clip_by_value(ic * (tf.pow(I, k) - tf.pow(ref_val, k)), ARG_CLIP_MIN, ARG_CLIP_MAX)) - 1.0)\n",
    "\n",
    "    def _term_sinh(self, I, k, ic, oc, ref_val):\n",
    "        return oc * tf.sinh(tf.clip_by_value(ic * (tf.pow(I, k) - tf.pow(ref_val, k)), ARG_CLIP_MIN, ARG_CLIP_MAX))\n",
    "\n",
    "    def _cap_exponents(self, k_val, i_val=None):\n",
    "        # Enforce k ∈ [K_MIN, K_MAX], i ∈ [I_MIN, I_MAX]\n",
    "        k_val = tf.clip_by_value(k_val, K_MIN, K_MAX)\n",
    "        if i_val is None:\n",
    "            return k_val\n",
    "        i_val = tf.clip_by_value(i_val, I_MIN, I_MAX)\n",
    "        return k_val, i_val\n",
    "\n",
    "    def call(self, I1, I2, I4, I6):\n",
    "        # Optional invariant normalization (off by default)\n",
    "        if NORMALIZE_INVARIANTS:\n",
    "            # Use stop_gradient to avoid leaking scale into grads\n",
    "            s = tf.stop_gradient(tf.maximum(tf.reduce_mean(I1), self.three))\n",
    "            I1 = I1 / s\n",
    "            I2 = I2 / (s * s)  # rough scale; preserves relative magnitudes\n",
    "            # I4, I6 keep as-is because they tie directly to fiber terms about λ1\n",
    "\n",
    "        # Convert raw -> positive then cap exponents\n",
    "        k1=1.0+tf.exp(self.raw_log_k1);  k1 = self._cap_exponents(k1)\n",
    "        k2=1.5+tf.exp(self.raw_log_k2);  k2 = self._cap_exponents(k2)\n",
    "        k3=1.0+tf.exp(self.raw_log_k3);  k3 = self._cap_exponents(k3)\n",
    "        k4=1.5+tf.exp(self.raw_log_k4);  k4 = self._cap_exponents(k4)\n",
    "        k5=1.0+tf.exp(self.raw_log_k5);  k5 = self._cap_exponents(k5)\n",
    "        k6=1.5+tf.exp(self.raw_log_k6);  k6 = self._cap_exponents(k6)\n",
    "        k7=1.0+tf.exp(self.raw_log_k7);  k7 = self._cap_exponents(k7)\n",
    "        k8=1.5+tf.exp(self.raw_log_k8);  k8 = self._cap_exponents(k8)\n",
    "        k9=1.0+tf.exp(self.raw_log_k9);  k9 = self._cap_exponents(k9)\n",
    "        k10=1.5+tf.exp(self.raw_log_k10); k10 = self._cap_exponents(k10)\n",
    "        k11=1.0+tf.exp(self.raw_log_k11); k11 = self._cap_exponents(k11)\n",
    "        k12=1.5+tf.exp(self.raw_log_k12); k12 = self._cap_exponents(k12)\n",
    "        k13=1.0+tf.exp(self.raw_log_k13); k13 = self._cap_exponents(k13)\n",
    "        k14=1.5+tf.exp(self.raw_log_k14); k14 = self._cap_exponents(k14)\n",
    "        k15=1.0+tf.exp(self.raw_log_k15); k15 = self._cap_exponents(k15)\n",
    "        k16=1.5+tf.exp(self.raw_log_k16); k16 = self._cap_exponents(k16)\n",
    "\n",
    "        i1=1.0+tf.exp(self.raw_log_i1); i1 = self._cap_exponents(i1, i1)[1]\n",
    "        i2=1.0+tf.exp(self.raw_log_i2); i2 = self._cap_exponents(i2, i2)[1]\n",
    "        i3=1.0+tf.exp(self.raw_log_i3); i3 = self._cap_exponents(i3, i3)[1]\n",
    "        i4=1.0+tf.exp(self.raw_log_i4); i4 = self._cap_exponents(i4, i4)[1]\n",
    "        i5=1.0+tf.exp(self.raw_log_i5); i5 = self._cap_exponents(i5, i5)[1]\n",
    "        i6=1.0+tf.exp(self.raw_log_i6); i6 = self._cap_exponents(i6, i6)[1]\n",
    "        i7=1.0+tf.exp(self.raw_log_i7); i7 = self._cap_exponents(i7, i7)[1]\n",
    "        i8=1.0+tf.exp(self.raw_log_i8); i8 = self._cap_exponents(i8, i8)[1]\n",
    "\n",
    "        a1=tf.exp(self.raw_log_a1); a2=tf.exp(self.raw_log_a2); a3=tf.exp(self.raw_log_a3); a4=tf.exp(self.raw_log_a4)\n",
    "        a5=tf.exp(self.raw_log_a5); a6=tf.exp(self.raw_log_a6); a7=tf.exp(self.raw_log_a7); a8=tf.exp(self.raw_log_a8)\n",
    "        a3_prime=tf.exp(self.raw_log_a3_prime); a4_prime=tf.exp(self.raw_log_a4_prime); a5_prime=tf.exp(self.raw_log_a5_prime)\n",
    "        a6_prime=tf.exp(self.raw_log_a6_prime); a7_prime=tf.exp(self.raw_log_a7_prime); a8_prime=tf.exp(self.raw_log_a8_prime)\n",
    "        b2=tf.exp(self.raw_log_b2); b3=tf.exp(self.raw_log_b3); b4=tf.exp(self.raw_log_b4); b5=tf.exp(self.raw_log_b5)\n",
    "        b6=tf.exp(self.raw_log_b6); b7=tf.exp(self.raw_log_b7); b8=tf.exp(self.raw_log_b8)\n",
    "        b3_prime=tf.exp(self.raw_log_b3_prime); b4_prime=tf.exp(self.raw_log_b4_prime); b5_prime=tf.exp(self.raw_log_b5_prime)\n",
    "        b6_prime=tf.exp(self.raw_log_b6_prime); b7_prime=tf.exp(self.raw_log_b7_prime); b8_prime=tf.exp(self.raw_log_b8_prime)\n",
    "\n",
    "        # b1 computed to satisfy normalization constraint without making it trainable\n",
    "        rhs = (b2 * k10) + (b4 * b4_prime * k12) + (b8 * b8_prime * k16)\n",
    "        lhs_sub = (b3 * b3_prime * k11) + (b7 * b7_prime * k15)\n",
    "        b1 = (rhs - lhs_sub) / (k9 + 1e-8)\n",
    "\n",
    "        W = tf.zeros_like(I1,dtype=tf.float64)\n",
    "        # Isotropic terms\n",
    "        W += self._term_power_law(I1,k1,i1,a1,self.three)\n",
    "        W += self._term_power_law(I2,k2,i2,a2,self.three)\n",
    "        W += self._term_exponential(I1,k3,i3,a3_prime,a3,self.three)\n",
    "        W += self._term_exponential(I2,k4,i4,a4_prime,a4,self.three)\n",
    "        W += self._term_cosh_minus_one_with_i(I1,k5,i5,a5_prime,a5,self.three)\n",
    "        W += self._term_cosh_minus_one_with_i(I2,k6,i6,a6_prime,a6,self.three)\n",
    "        W += self._term_sinh_with_i(I1,k7,i7,a7_prime,a7,self.three)\n",
    "        W += self._term_sinh_with_i(I2,k8,i8,a8_prime,a8,self.three)\n",
    "        # Anisotropic/fiber-like terms\n",
    "        W += self._term_identity_scaled(I4,k9,b1,self.one)\n",
    "        W += self._term_identity_scaled(I6,k10,b2,self.one)\n",
    "        W += self._term_exponential_no_i(I4,k11,b3_prime,b3,self.one)\n",
    "        W += self._term_exponential_no_i(I6,k12,b4_prime,b4,self.one)\n",
    "        W += self._term_cosh_minus_one(I4,k13,b5_prime,b5,self.one)\n",
    "        W += self._term_cosh_minus_one(I6,k14,b6_prime,b6,self.one)\n",
    "        W += self._term_sinh(I4,k15,b7_prime,b7,self.one)\n",
    "        W += self._term_sinh(I6,k16,b8_prime,b8,self.one)\n",
    "        return W\n",
    "\n",
    "@tf.function\n",
    "def get_invariants_tf(lambda1, lambda2, lambda3):\n",
    "    # Guard against non-physical stretches and numerically tiny values\n",
    "    min_lambda_val = tf.constant(1e-6, dtype=tf.float64)\n",
    "    # Assert positivity (will raise during eager; in graph, inserts runtime checks)\n",
    "    tf.debugging.assert_greater(lambda1, 0.0, message=\"λ1 must be > 0\")\n",
    "    tf.debugging.assert_greater(lambda2, 0.0, message=\"λ2 must be > 0\")\n",
    "    tf.debugging.assert_greater(lambda3, 0.0, message=\"λ3 must be > 0\")\n",
    "\n",
    "    lambda1 = tf.maximum(lambda1, min_lambda_val)\n",
    "    lambda2 = tf.maximum(lambda2, min_lambda_val)\n",
    "    lambda3 = tf.maximum(lambda3, min_lambda_val)\n",
    "\n",
    "    l1s = tf.pow(lambda1, 2.0); l2s = tf.pow(lambda2, 2.0); l3s = tf.pow(lambda3, 2.0)\n",
    "    I1 = l1s + l2s + l3s\n",
    "    I2 = tf.pow(lambda1 * lambda2, 2.0) + tf.pow(lambda2 * lambda3, 2.0) + tf.pow(lambda3 * lambda1, 2.0)\n",
    "    I4 = l1s; I6 = 1.0 / l1s\n",
    "    return I1, I2, I4, I6\n",
    "\n",
    "# --- Stress calculators with exact-zero clamp at λ=1 ---\n",
    "\n",
    "@tf.function\n",
    "def _calculate_raw_uniaxial_p11(l1, model):\n",
    "    \"\"\"Helper to compute uncorrected uniaxial stress.\"\"\"\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        l1_t = tf.identity(l1)\n",
    "        l2_t = tf.pow(l1_t, -0.5)\n",
    "        l3_t = tf.pow(l1_t, -0.5)\n",
    "        tape.watch([l1_t, l2_t])\n",
    "        I1, I2, I4, I6 = get_invariants_tf(l1_t, l2_t, l3_t)\n",
    "        W = model(I1, I2, I4, I6)\n",
    "    dWdl1 = tape.gradient(W, l1_t)\n",
    "    dWdl2 = tape.gradient(W, l2_t)\n",
    "    del tape\n",
    "    p = l2_t * dWdl2\n",
    "    P11 = dWdl1 - p / l1_t\n",
    "    return P11\n",
    "\n",
    "@tf.function\n",
    "def predict_uniaxial_p11(lambda1, model):\n",
    "    \"\"\"Compute uniaxial stress, ensure P11=0 at λ1=1 within tolerance.\"\"\"\n",
    "    p11_raw = _calculate_raw_uniaxial_p11(lambda1, model)\n",
    "    # Get scalar offset at exactly 1.0 (compute as scalar, subtract via broadcasting)\n",
    "    p11_offset = _calculate_raw_uniaxial_p11(tf.constant([1.0], dtype=tf.float64), model)[0]\n",
    "    p11 = p11_raw - p11_offset\n",
    "\n",
    "    # Clamp tiny residuals to zero to prevent -1e-8 type artifacts\n",
    "    p11 = tf.where(tf.abs(p11) < NEAR_ZERO_TOL, tf.zeros_like(p11), p11)\n",
    "    # If any exact λ1==1 in the batch, hard set to 0\n",
    "    if lambda1.shape.rank is not None:\n",
    "        p11 = tf.where(tf.abs(lambda1 - 1.0) < NEAR_ZERO_TOL, tf.zeros_like(p11), p11)\n",
    "    return p11\n",
    "\n",
    "@tf.function\n",
    "def get_raw_biaxial_stresses(l1, l2, model):\n",
    "    \"\"\"Helper to compute uncorrected biaxial stresses.\"\"\"\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        l1_t = tf.identity(l1); l2_t = tf.identity(l2)\n",
    "        l3_t = 1.0 / (l1_t * l2_t)\n",
    "        tape.watch([l1_t, l2_t, l3_t])\n",
    "        I1, I2, I4, I6 = get_invariants_tf(l1_t, l2_t, l3_t)\n",
    "        W = model(I1, I2, I4, I6)\n",
    "    dWdl1 = tape.gradient(W, l1_t); dWdl2 = tape.gradient(W, l2_t); dWdl3 = tape.gradient(W, l3_t)\n",
    "    del tape\n",
    "    p = l3_t * dWdl3\n",
    "    P11 = dWdl1 - p / l1_t\n",
    "    P22 = dWdl2 - p / l2_t\n",
    "    return P11, P22\n",
    "\n",
    "@tf.function\n",
    "def get_corrected_biaxial_stresses(l1, l2, model):\n",
    "    \"\"\"Compute biaxial stresses, P=0 at (1,1), then clamp tiny residuals.\"\"\"\n",
    "    p11_raw, p22_raw = get_raw_biaxial_stresses(l1, l2, model)\n",
    "    off_p11, off_p22 = get_raw_biaxial_stresses(tf.constant(1.0, tf.float64), tf.constant(1.0, tf.float64), model)\n",
    "    p11 = p11_raw - off_p11\n",
    "    p22 = p22_raw - off_p22\n",
    "\n",
    "    p11 = tf.where(tf.abs(p11) < NEAR_ZERO_TOL, tf.zeros_like(p11), p11)\n",
    "    p22 = tf.where(tf.abs(p22) < NEAR_ZERO_TOL, tf.zeros_like(p22), p22)\n",
    "\n",
    "    # If exactly at undeformed axes, force zeros\n",
    "    p11 = tf.where((tf.abs(l1 - 1.0) < NEAR_ZERO_TOL) & (tf.abs(l2 - 1.0) < NEAR_ZERO_TOL), tf.zeros_like(p11), p11)\n",
    "    p22 = tf.where((tf.abs(l1 - 1.0) < NEAR_ZERO_TOL) & (tf.abs(l2 - 1.0) < NEAR_ZERO_TOL), tf.zeros_like(p22), p22)\n",
    "    return p11, p22\n",
    "\n",
    "@tf.function\n",
    "def predict_biaxial_p22(lambda2, model):\n",
    "    \"\"\"Find P22 for a given lambda2 where P11=0 (bisection), with zero clamp at λ2=1.\"\"\"\n",
    "    l1_min = tf.ones_like(lambda2, dtype=tf.float64) * 0.5\n",
    "    l1_max = tf.ones_like(lambda2, dtype=tf.float64) * 1.5\n",
    "    for _ in range(25):\n",
    "        l1_mid = (l1_min + l1_max) / 2.0\n",
    "        p11_mid, _ = get_corrected_biaxial_stresses(l1_mid, lambda2, model)\n",
    "        p11_min, _ = get_corrected_biaxial_stresses(l1_min, lambda2, model)\n",
    "        is_same_sign = tf.sign(p11_mid) == tf.sign(p11_min)\n",
    "        l1_min = tf.where(is_same_sign, l1_mid, l1_min)\n",
    "        l1_max = tf.where(is_same_sign, l1_max, l1_mid)\n",
    "    final_l1 = tf.stop_gradient((l1_min + l1_max) / 2.0)\n",
    "    _, P22_final = get_corrected_biaxial_stresses(final_l1, lambda2, model)\n",
    "    # Clamp tiny residuals\n",
    "    P22_final = tf.where(tf.abs(P22_final) < NEAR_ZERO_TOL, tf.zeros_like(P22_final), P22_final)\n",
    "    P22_final = tf.where(tf.abs(lambda2 - 1.0) < NEAR_ZERO_TOL, tf.zeros_like(P22_final), P22_final)\n",
    "    return P22_final\n",
    "\n",
    "# --- Experimental Data ---\n",
    "exp_data_raw_uniaxial_cnf = np.array([\n",
    "    [1.0000,0],[1.0708,0.3840],[1.2017,0.8987],[1.3125,1.1814],[1.4000,1.4093],\n",
    "    [1.5125,1.6456],[1.6017,1.8608],[1.7125,2.1055],[1.8008,2.3122],[1.8883,2.5570],\n",
    "    [1.9767,2.7848],[2.0883,3.1519],[2.1992,3.5274],[2.2867,3.8354],[2.3975,4.2532],\n",
    "    [2.4383,4.4304],[2.4858,4.5949]\n",
    "])\n",
    "exp_data_raw_biaxial_cnf = np.array([\n",
    "    [1.0000,0],[1.3208,1.0506],[1.4017,1.2068],[1.5092,1.3840],[1.5983,1.5401],\n",
    "    [1.7017,1.6835],[1.7842,1.7848],[1.8967,1.9662],[1.9792,2.1181],[2.0858,2.2911],\n",
    "    [2.1708,2.4599],[2.2783,2.6962],[2.3825,2.9409],[2.4225,3.0549],[2.4867,3.2236]\n",
    "])\n",
    "\n",
    "uniaxial_l1, uniaxial_p11 = [tf.constant(c, dtype=tf.float64) for c in exp_data_raw_uniaxial_cnf.T]\n",
    "biaxial_l2, biaxial_p22 = [tf.constant(c, dtype=tf.float64) for c in exp_data_raw_biaxial_cnf.T]\n",
    "\n",
    "# --- Model & Training Setup ---\n",
    "model = StrainEnergyANN_Layered_TF()\n",
    "\n",
    "initial_learning_rate = 0.005  # your choice retained\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=2000, decay_rate=0.9, staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "loss_weight_biaxial = tf.constant(1.0, dtype=tf.float64)\n",
    "loss_history = []; learning_rate_history = []\n",
    "\n",
    "# Regularizer strength (mild)\n",
    "L2_REG = tf.constant(1e-6, dtype=tf.float64)\n",
    "huber = tf.keras.losses.Huber(delta=0.05)  # robust to outliers\n",
    "\n",
    "@tf.function\n",
    "def train_step(uniaxial_l1, uniaxial_p11, biaxial_l2, biaxial_p22, loss_weight):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Uniaxial prediction\n",
    "        p11_pred = predict_uniaxial_p11(uniaxial_l1, model)\n",
    "        # Relative error with epsilon in denom and robust Huber\n",
    "        rel_err_1 = (p11_pred[1:] - uniaxial_p11[1:]) / (tf.abs(uniaxial_p11[1:]) + REL_DEN_EPS)\n",
    "        loss1 = tf.reduce_mean(huber(tf.zeros_like(rel_err_1), rel_err_1))\n",
    "\n",
    "        # Biaxial prediction\n",
    "        p22_pred = predict_biaxial_p22(biaxial_l2, model)\n",
    "        rel_err_2 = (p22_pred[1:] - biaxial_p22[1:]) / (tf.abs(biaxial_p22[1:]) + REL_DEN_EPS)\n",
    "        loss2 = tf.reduce_mean(huber(tf.zeros_like(rel_err_2), rel_err_2))\n",
    "\n",
    "        # Mild L2 regularization to discourage huge params (applies to raw logs)\n",
    "        l2 = tf.add_n([tf.reduce_sum(tf.square(v)) for v in model.trainable_variables])\n",
    "        reg = L2_REG * l2\n",
    "\n",
    "        total_loss = loss1 + loss_weight * loss2 + reg\n",
    "\n",
    "        # Safety: check finiteness\n",
    "        tf.debugging.assert_all_finite(total_loss, \"Non-finite loss detected\")\n",
    "\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    # Clip gradients to [-1, 1] (your scheme) – preserves your structure\n",
    "    grads = [tf.clip_by_value(g, -1.0, 1.0) if g is not None else g for g in grads]\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return total_loss, loss1, loss2, reg\n",
    "\n",
    "# --- Training ---\n",
    "epochs = 300  # retained\n",
    "for epoch in range(epochs):\n",
    "    total_loss, loss1, loss2, reg = train_step(uniaxial_l1, uniaxial_p11, biaxial_l2, biaxial_p22, loss_weight_biaxial)\n",
    "    loss_history.append(float(total_loss.numpy()))\n",
    "    current_lr = lr_schedule(optimizer.iterations)\n",
    "    learning_rate_history.append(float(current_lr.numpy()))\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch:5d}, Loss: {total_loss:.5f} (Uni: {loss1:.5f}, Bi: {loss2:.5f}, Reg: {reg:.5e}), LR: {current_lr:.6f}\")\n",
    "\n",
    "print(f\"\\nTraining finished. Final Loss: {loss_history[-1]:.5f}\")\n",
    "\n",
    "# --- Visualization ---\n",
    "l1_plot = np.linspace(1.0, exp_data_raw_uniaxial_cnf[:, 0].max(), 100)\n",
    "p11_pred_plot = predict_uniaxial_p11(tf.constant(l1_plot, dtype=tf.float64), model).numpy()\n",
    "l2_plot = np.linspace(1.0, exp_data_raw_biaxial_cnf[:, 0].max(), 100)\n",
    "p22_pred_plot = predict_biaxial_p22(tf.constant(l2_plot, dtype=tf.float64), model).numpy()\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "axes[0].plot(exp_data_raw_uniaxial_cnf[:,0], exp_data_raw_uniaxial_cnf[:,1], 'ro', label='Experimental Data')\n",
    "axes[0].plot(l1_plot, p11_pred_plot, 'b-', lw=2, label='Model Prediction (P11)')\n",
    "axes[0].set_xlabel('Stretch $\\\\lambda_1$', fontsize=12); axes[0].set_ylabel('Nominal Stress $P_{11}$ (MPa)', fontsize=12)\n",
    "axes[0].set_title('Task 1: Uniaxial Tension Fit', fontsize=14); axes[0].legend(); axes[0].set_ylim(bottom=0); axes[0].set_xlim(left=1)\n",
    "\n",
    "axes[1].plot(exp_data_raw_biaxial_cnf[:,0], exp_data_raw_biaxial_cnf[:,1], 'go', label='Experimental Data')\n",
    "axes[1].plot(l2_plot, p22_pred_plot, 'b-', lw=2, label='Model Prediction (P22)')\n",
    "axes[1].set_xlabel('Stretch $\\\\lambda_2$', fontsize=12); axes[1].set_ylabel('Nominal Stress $P_{22}$ (MPa)', fontsize=12)\n",
    "axes[1].set_title('Task 2: Biaxial (Planar) Tension Fit', fontsize=14); axes[1].legend(); axes[1].set_ylim(bottom=0); axes[1].set_xlim(left=1)\n",
    "\n",
    "ax2 = axes[2]\n",
    "ax2.plot(loss_history, label='Total Loss', color='darkblue'); ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Total Loss (robust rel.)', fontsize=12, color='darkblue'); ax2.set_title('Training History', fontsize=14)\n",
    "ax2.set_yscale('log'); ax2.tick_params(axis='y', labelcolor='darkblue'); ax2.grid(True, which=\"both\", ls=\"--\")\n",
    "\n",
    "ax3 = ax2.twinx()\n",
    "ax3.plot(learning_rate_history, label='Learning Rate', color='red', linestyle='--')\n",
    "ax3.set_ylabel('Learning Rate', fontsize=12, color='red'); ax3.tick_params(axis='y', labelcolor='red')\n",
    "fig.tight_layout(); plt.show()\n",
    "\n",
    "# --- Final Check (should be exactly zero within tolerance) ---\n",
    "p11_final_check = predict_uniaxial_p11(tf.constant([1.0], dtype=tf.float64), model)\n",
    "p22_final_check = predict_biaxial_p22(tf.constant([1.0], dtype=tf.float64), model)\n",
    "print(\"\\nFinal check (clamped to zero within tolerance):\")\n",
    "print(f\"P11 at lambda1=1: {p11_final_check.numpy()[0]:.6e}\")\n",
    "print(f\"P22 at lambda2=1: {p22_final_check.numpy()[0]:.6e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd573d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chicken (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
