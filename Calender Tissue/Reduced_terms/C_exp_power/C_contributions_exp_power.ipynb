{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow pandas scipy -q\n",
        "!pip install -U \"seaborn\" --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfEGMjhdUs-v",
        "outputId": "0e1ae717-ba9c-4a97-94f8-72e93467629d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBkPJsQJPSry",
        "outputId": "41dcb00e-c0ff-4861-9c2c-d6f4556e476b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: /CPU:0 for model type: exp_power\n",
            "--- Training exp_power model for Calender Tissue ---\n",
            "Epoch     0, Loss: 305114545.61564 (Uni_rel: 22008962.47473, Bi_rel: 23512810.40599), LR: 0.003000\n",
            "Epoch  1000, Loss: 0.25703 (Uni_rel: 0.07580, Bi_rel: 0.01213), LR: 0.003000\n",
            "Epoch  2000, Loss: 0.06321 (Uni_rel: 0.02831, Bi_rel: 0.00237), LR: 0.002700\n",
            "Epoch  3000, Loss: 0.04767 (Uni_rel: 0.02095, Bi_rel: 0.00164), LR: 0.002700\n",
            "Epoch  4000, Loss: 0.03932 (Uni_rel: 0.01725, Bi_rel: 0.00134), LR: 0.002430\n",
            "Epoch  5000, Loss: 0.02551 (Uni_rel: 0.01403, Bi_rel: 0.00069), LR: 0.002430\n",
            "Epoch  6000, Loss: 0.01328 (Uni_rel: 0.00602, Bi_rel: 0.00053), LR: 0.002187\n",
            "Epoch  7000, Loss: 0.01175 (Uni_rel: 0.00131, Bi_rel: 0.00083), LR: 0.002187\n",
            "Epoch  8000, Loss: 0.00848 (Uni_rel: 0.00136, Bi_rel: 0.00057), LR: 0.001968\n",
            "Epoch  9000, Loss: 0.00768 (Uni_rel: 0.00121, Bi_rel: 0.00051), LR: 0.001968\n",
            "Epoch 10000, Loss: 0.00830 (Uni_rel: 0.00095, Bi_rel: 0.00059), LR: 0.001771\n",
            "Epoch 11000, Loss: 0.01006 (Uni_rel: 0.00069, Bi_rel: 0.00076), LR: 0.001771\n",
            "Epoch 12000, Loss: 0.01417 (Uni_rel: 0.00037, Bi_rel: 0.00113), LR: 0.001594\n",
            "Epoch 13000, Loss: 0.02040 (Uni_rel: 0.00014, Bi_rel: 0.00165), LR: 0.001594\n",
            "Epoch 14000, Loss: 0.60571 (Uni_rel: 0.00156, Bi_rel: 0.03869), LR: 0.001435\n",
            "Epoch 15000, Loss: 0.36036 (Uni_rel: 0.00172, Bi_rel: 0.02673), LR: 0.001435\n",
            "Epoch 16000, Loss: 1.29064 (Uni_rel: 0.00071, Bi_rel: 0.08312), LR: 0.001291\n",
            "Epoch 17000, Loss: 0.04347 (Uni_rel: 0.00008, Bi_rel: 0.00359), LR: 0.001291\n",
            "Epoch 18000, Loss: 0.03469 (Uni_rel: 0.00006, Bi_rel: 0.00287), LR: 0.001162\n",
            "Epoch 19000, Loss: 0.03215 (Uni_rel: 0.00005, Bi_rel: 0.00266), LR: 0.001162\n",
            "Epoch 20000, Loss: 0.16298 (Uni_rel: 0.00014, Bi_rel: 0.01282), LR: 0.001046\n",
            "Epoch 21000, Loss: 0.08819 (Uni_rel: 0.00017, Bi_rel: 0.00681), LR: 0.001046\n",
            "Epoch 22000, Loss: 0.10726 (Uni_rel: 0.00014, Bi_rel: 0.00880), LR: 0.000941\n",
            "Epoch 23000, Loss: 0.04683 (Uni_rel: 0.00006, Bi_rel: 0.00387), LR: 0.000941\n",
            "Epoch 24000, Loss: 0.03660 (Uni_rel: 0.00005, Bi_rel: 0.00303), LR: 0.000847\n",
            "Epoch 25000, Loss: 0.03292 (Uni_rel: 0.00005, Bi_rel: 0.00272), LR: 0.000847\n",
            "Epoch 26000, Loss: 0.03379 (Uni_rel: 0.00006, Bi_rel: 0.00262), LR: 0.000763\n",
            "Epoch 27000, Loss: 0.13350 (Uni_rel: 0.00017, Bi_rel: 0.01034), LR: 0.000763\n",
            "Epoch 28000, Loss: 0.08136 (Uni_rel: 0.00014, Bi_rel: 0.00631), LR: 0.000686\n",
            "Epoch 29000, Loss: 0.13675 (Uni_rel: 0.00038, Bi_rel: 0.01121), LR: 0.000686\n",
            "\n",
            "Training finished. Final Loss: 0.05908\n",
            "\n",
            "Calculating contributions for uniaxial...\n",
            "  - Calculated contribution for term: Pow(I1)\n",
            "  - Calculated contribution for term: Pow(I2)\n",
            "  - Calculated contribution for term: Exp(I1)\n",
            "  - Calculated contribution for term: Exp(I2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function predict_uniaxial_p11 at 0x7e6ef8f66e80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Calculated contribution for term: Pow(I4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function predict_uniaxial_p11 at 0x7e6ef8f66e80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Calculated contribution for term: Pow(I6)\n",
            "  - Calculated contribution for term: Exp(I4)\n",
            "  - Calculated contribution for term: Exp(I6)\n",
            "\n",
            "Calculating contributions for biaxial...\n",
            "  - Calculated contribution for term: Pow(I1)\n",
            "  - Calculated contribution for term: Pow(I2)\n",
            "  - Calculated contribution for term: Exp(I1)\n",
            "  - Calculated contribution for term: Exp(I2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function predict_biaxial_p22 at 0x7e6ef8f67c40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Calculated contribution for term: Pow(I4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function predict_biaxial_p22 at 0x7e6ef8f67c40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Calculated contribution for term: Pow(I6)\n",
            "  - Calculated contribution for term: Exp(I4)\n",
            "  - Calculated contribution for term: Exp(I6)\n",
            "\n",
            "Successfully saved stress contributions to 'calender_contributions_exp_power.csv'\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorflow pandas scipy -q\n",
        "# !pip install -U \"seaborn\" --quiet\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import savemat\n",
        "\n",
        "# --- Main Configuration ---\n",
        "# CHANGE THIS VARIABLE to 'exp_power' or 'sinh_cosh_power' before running\n",
        "MODEL_TYPE = 'exp_power' # or 'sinh_cosh_power'\n",
        "\n",
        "# --- Setup, Model Definitions, Helper/Physics Functions ---\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "DEVICE = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
        "print(f\"Using device: {DEVICE} for model type: {MODEL_TYPE}\")\n",
        "\n",
        "ARG_CLIP_MIN = tf.constant(-10.0, dtype=tf.float64)\n",
        "ARG_CLIP_MAX = tf.constant(10.0, dtype=tf.float64)\n",
        "\n",
        "@tf.function\n",
        "def smooth_relu(x, beta=20.0):\n",
        "    return tf.nn.softplus(beta * x) / beta\n",
        "\n",
        "# --- Model for Exponential Power Terms Only ---\n",
        "class StrainEnergy_ExpPower_C(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"StrainEnergy_ExpPower_C\")\n",
        "        self.raw_log_k1=self.add_weight(name=\"raw_log_k1\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_k2=self.add_weight(name=\"raw_log_k2\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_i1=self.add_weight(name=\"raw_log_i1\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_i2=self.add_weight(name=\"raw_log_i2\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_a1=self.add_weight(name=\"raw_log_a1\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_a2=self.add_weight(name=\"raw_log_a2\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_k3=self.add_weight(name=\"raw_log_k3\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_k4=self.add_weight(name=\"raw_log_k4\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_i3=self.add_weight(name=\"raw_log_i3\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_i4=self.add_weight(name=\"raw_log_i4\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_a3=self.add_weight(name=\"raw_log_a3\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_a4=self.add_weight(name=\"raw_log_a4\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_a3_prime=self.add_weight(name=\"raw_log_a3_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
        "        self.raw_log_a4_prime=self.add_weight(name=\"raw_log_a4_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
        "        self.raw_log_k9=self.add_weight(name=\"raw_log_k9\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_k10=self.add_weight(name=\"raw_log_k10\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_b1=self.add_weight(name=\"raw_log_b1\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.01)),trainable=True)\n",
        "        self.raw_log_b2=self.add_weight(name=\"raw_log_b2\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.01)),trainable=True)\n",
        "        self.raw_log_k11=self.add_weight(name=\"raw_log_k11\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_k12=self.add_weight(name=\"raw_log_k12\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_b3=self.add_weight(name=\"raw_log_b3\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_b4=self.add_weight(name=\"raw_log_b4\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_b3_prime=self.add_weight(name=\"raw_log_b3_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
        "        self.raw_log_b4_prime=self.add_weight(name=\"raw_log_b4_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
        "        self.three=tf.constant(3.0,dtype=tf.float64); self.one=tf.constant(1.0,dtype=tf.float64); self.pow_base_epsilon=tf.constant(1e-8,dtype=tf.float64)\n",
        "    def _term_power_law(self, I, k, i, c, ref_val): return c * tf.pow(smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k)) + self.pow_base_epsilon, i)\n",
        "    def _term_exponential(self, I, k, i, ic, oc, ref_val): return oc * (tf.exp(tf.clip_by_value(ic * tf.pow(smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k)) + self.pow_base_epsilon, i), ARG_CLIP_MIN, ARG_CLIP_MAX)) - 1.0)\n",
        "    def _term_identity_scaled(self, I, k, c, ref_val): return c * (tf.pow(I, k) - tf.pow(ref_val, k))\n",
        "    def _term_exponential_no_i(self, I, k, ic, oc, ref_val): return oc * (tf.exp(tf.clip_by_value(ic * (tf.pow(I, k) - tf.pow(ref_val, k)), ARG_CLIP_MIN, ARG_CLIP_MAX)) - 1.0)\n",
        "    def call(self, I1, I2, I4, I6):\n",
        "        k1=1.0+tf.exp(self.raw_log_k1); k2=1.5+tf.exp(self.raw_log_k2); k3=1.0+tf.exp(self.raw_log_k3); k4=1.5+tf.exp(self.raw_log_k4)\n",
        "        i1=1.0+tf.exp(self.raw_log_i1); i2=1.0+tf.exp(self.raw_log_i2); i3=1.0+tf.exp(self.raw_log_i3); i4=1.0+tf.exp(self.raw_log_i4)\n",
        "        a1=tf.exp(self.raw_log_a1); a2=tf.exp(self.raw_log_a2); a3=tf.exp(self.raw_log_a3); a4=tf.exp(self.raw_log_a4)\n",
        "        a3_prime=tf.exp(self.raw_log_a3_prime); a4_prime=tf.exp(self.raw_log_a4_prime)\n",
        "        k9=1.0+tf.exp(self.raw_log_k9); k10=1.5+tf.exp(self.raw_log_k10); k11=1.0+tf.exp(self.raw_log_k11); k12=1.5+tf.exp(self.raw_log_k12)\n",
        "        b1=tf.exp(self.raw_log_b1); b2=tf.exp(self.raw_log_b2); b3=tf.exp(self.raw_log_b3); b4=tf.exp(self.raw_log_b4)\n",
        "        b3_prime=tf.exp(self.raw_log_b3_prime); b4_prime=tf.exp(self.raw_log_b4_prime)\n",
        "        W = tf.zeros_like(I1, dtype=tf.float64)\n",
        "        W += self._term_power_law(I1, k1, i1, a1, self.three); W += self._term_power_law(I2, k2, i2, a2, self.three)\n",
        "        W += self._term_exponential(I1, k3, i3, a3_prime, a3, self.three); W += self._term_exponential(I2, k4, i4, a4_prime, a4, self.three)\n",
        "        W += self._term_identity_scaled(I4, k9, b1, self.one); W += self._term_identity_scaled(I6, k10, b2, self.one)\n",
        "        W += self._term_exponential_no_i(I4, k11, b3_prime, b3, self.one); W += self._term_exponential_no_i(I6, k12, b4_prime, b4, self.one)\n",
        "        return W\n",
        "\n",
        "# --- Model for Sinh-Cosh Power Terms Only ---\n",
        "class StrainEnergy_SinhCoshPower_C(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__(name=\"StrainEnergy_SinhCoshPower_C\")\n",
        "        self.raw_log_k1=self.add_weight(name=\"raw_log_k1\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_k2=self.add_weight(name=\"raw_log_k2\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_i1=self.add_weight(name=\"raw_log_i1\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_i2=self.add_weight(name=\"raw_log_i2\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_a1=self.add_weight(name=\"raw_log_a1\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_a2=self.add_weight(name=\"raw_log_a2\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_k5=self.add_weight(name=\"raw_log_k5\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_k6=self.add_weight(name=\"raw_log_k6\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_i5=self.add_weight(name=\"raw_log_i5\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_i6=self.add_weight(name=\"raw_log_i6\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_a5=self.add_weight(name=\"raw_log_a5\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_a6=self.add_weight(name=\"raw_log_a6\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_a5_prime=self.add_weight(name=\"raw_log_a5_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
        "        self.raw_log_a6_prime=self.add_weight(name=\"raw_log_a6_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
        "        self.raw_log_k7=self.add_weight(name=\"raw_log_k7\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_k8=self.add_weight(name=\"raw_log_k8\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_i7=self.add_weight(name=\"raw_log_i7\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_i8=self.add_weight(name=\"raw_log_i8\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_a7=self.add_weight(name=\"raw_log_a7\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_a8=self.add_weight(name=\"raw_log_a8\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_a7_prime=self.add_weight(name=\"raw_log_a7_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
        "        self.raw_log_a8_prime=self.add_weight(name=\"raw_log_a8_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
        "        self.raw_log_k13=self.add_weight(name=\"raw_log_k13\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_k14=self.add_weight(name=\"raw_log_k14\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_b5=self.add_weight(name=\"raw_log_b5\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_b6=self.add_weight(name=\"raw_log_b6\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_b5_prime=self.add_weight(name=\"raw_log_b5_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
        "        self.raw_log_b6_prime=self.add_weight(name=\"raw_log_b6_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
        "        self.raw_log_k15=self.add_weight(name=\"raw_log_k15\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_k16=self.add_weight(name=\"raw_log_k16\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
        "        self.raw_log_b7=self.add_weight(name=\"raw_log_b7\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_b8=self.add_weight(name=\"raw_log_b8\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
        "        self.raw_log_b7_prime=self.add_weight(name=\"raw_log_b7_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
        "        self.raw_log_b8_prime=self.add_weight(name=\"raw_log_b8_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
        "        self.three=tf.constant(3.0,dtype=tf.float64); self.one=tf.constant(1.0,dtype=tf.float64); self.pow_base_epsilon=tf.constant(1e-8,dtype=tf.float64)\n",
        "    def _term_power_law(self, I, k, i, c, ref_val): return c * tf.pow(smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k)) + self.pow_base_epsilon, i)\n",
        "    def _term_cosh_minus_one_with_i(self, I, k, i, ic, oc, ref_val): return oc * (tf.cosh(tf.clip_by_value(ic * tf.pow(smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k)) + self.pow_base_epsilon, i), ARG_CLIP_MIN, ARG_CLIP_MAX)) - 1.0)\n",
        "    def _term_sinh_with_i(self, I, k, i, ic, oc, ref_val): return oc * tf.sinh(tf.clip_by_value(ic * tf.pow(smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k)) + self.pow_base_epsilon, i), ARG_CLIP_MIN, ARG_CLIP_MAX))\n",
        "    def _term_cosh_minus_one(self, I, k, ic, oc, ref_val): return oc * (tf.cosh(tf.clip_by_value(ic * (tf.pow(I, k) - tf.pow(ref_val, k)), ARG_CLIP_MIN, ARG_CLIP_MAX)) - 1.0)\n",
        "    def _term_sinh(self, I, k, ic, oc, ref_val): return oc * tf.sinh(tf.clip_by_value(ic * (tf.pow(I, k) - tf.pow(ref_val, k)), ARG_CLIP_MIN, ARG_CLIP_MAX))\n",
        "    def call(self, I1, I2, I4, I6):\n",
        "        k1=1.0+tf.exp(self.raw_log_k1);k2=1.5+tf.exp(self.raw_log_k2);k5=1.0+tf.exp(self.raw_log_k5);k6=1.5+tf.exp(self.raw_log_k6);k7=1.0+tf.exp(self.raw_log_k7);k8=1.5+tf.exp(self.raw_log_k8);k13=1.0+tf.exp(self.raw_log_k13);k14=1.5+tf.exp(self.raw_log_k14);k15=1.0+tf.exp(self.raw_log_k15);k16=1.5+tf.exp(self.raw_log_k16)\n",
        "        i1=1.0+tf.exp(self.raw_log_i1);i2=1.0+tf.exp(self.raw_log_i2);i5=1.0+tf.exp(self.raw_log_i5);i6=1.0+tf.exp(self.raw_log_i6);i7=1.0+tf.exp(self.raw_log_i7);i8=1.0+tf.exp(self.raw_log_i8)\n",
        "        a1=tf.exp(self.raw_log_a1);a2=tf.exp(self.raw_log_a2);a5=tf.exp(self.raw_log_a5);a6=tf.exp(self.raw_log_a6);a7=tf.exp(self.raw_log_a7);a8=tf.exp(self.raw_log_a8)\n",
        "        a5_prime=tf.exp(self.raw_log_a5_prime);a6_prime=tf.exp(self.raw_log_a6_prime);a7_prime=tf.exp(self.raw_log_a7_prime);a8_prime=tf.exp(self.raw_log_a8_prime)\n",
        "        b5=tf.exp(self.raw_log_b5);b6=tf.exp(self.raw_log_b6);b7=tf.exp(self.raw_log_b7);b8=tf.exp(self.raw_log_b8)\n",
        "        b5_prime=tf.exp(self.raw_log_b5_prime);b6_prime=tf.exp(self.raw_log_b6_prime);b7_prime=tf.exp(self.raw_log_b7_prime);b8_prime=tf.exp(self.raw_log_b8_prime)\n",
        "        W = tf.zeros_like(I1,dtype=tf.float64)\n",
        "        W += self._term_power_law(I1,k1,i1,a1,self.three); W += self._term_power_law(I2,k2,i2,a2,self.three)\n",
        "        W += self._term_cosh_minus_one_with_i(I1,k5,i5,a5_prime,a5,self.three); W += self._term_cosh_minus_one_with_i(I2,k6,i6,a6_prime,a6,self.three)\n",
        "        W += self._term_sinh_with_i(I1,k7,i7,a7_prime,a7,self.three); W += self._term_sinh_with_i(I2,k8,i8,a8_prime,a8,self.three)\n",
        "        W += self._term_cosh_minus_one(I4,k13,b5_prime,b5,self.one); W += self._term_cosh_minus_one(I6,k14,b6_prime,b6,self.one)\n",
        "        W += self._term_sinh(I4,k15,b7_prime,b7,self.one); W += self._term_sinh(I6,k16,b8_prime,b8,self.one)\n",
        "        return W\n",
        "\n",
        "# --- Universal Physics, Training, and Contribution Calculation Functions ---\n",
        "@tf.function\n",
        "def get_invariants_tf(lambda1, lambda2, lambda3):\n",
        "    min_lambda_val = tf.constant(1e-6, dtype=tf.float64)\n",
        "    lambda1 = tf.maximum(lambda1, min_lambda_val); lambda2 = tf.maximum(lambda2, min_lambda_val); lambda3 = tf.maximum(lambda3, min_lambda_val)\n",
        "    l1s = tf.pow(lambda1, 2.0); l2s = tf.pow(lambda2, 2.0); l3s = tf.pow(lambda3, 2.0)\n",
        "    I1 = l1s + l2s + l3s; I2 = tf.pow(lambda1*lambda2, 2.0) + tf.pow(lambda2*lambda3, 2.0) + tf.pow(lambda3*lambda1, 2.0)\n",
        "    I4 = l1s; I6 = 1/l1s\n",
        "    return I1, I2, I4, I6\n",
        "@tf.function\n",
        "def _calculate_raw_uniaxial_p11(l1, model, W_func):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        l1_t, l2_t, l3_t = l1, tf.pow(l1, -0.5), tf.pow(l1, -0.5)\n",
        "        tape.watch([l1_t, l2_t])\n",
        "        I1, I2, I4, I6 = get_invariants_tf(l1_t, l2_t, l3_t)\n",
        "        W = W_func(I1, I2, I4, I6)\n",
        "    dWdl1 = tape.gradient(W, l1_t); dWdl2 = tape.gradient(W, l2_t)\n",
        "    del tape\n",
        "    p = l2_t * dWdl2\n",
        "    return dWdl1 - p / l1_t\n",
        "@tf.function\n",
        "def predict_uniaxial_p11(lambda1, model, W_func):\n",
        "    p11_raw = _calculate_raw_uniaxial_p11(lambda1, model, W_func)\n",
        "    p11_offset = _calculate_raw_uniaxial_p11(tf.constant([1.0], dtype=tf.float64), model, W_func)\n",
        "    return p11_raw - p11_offset\n",
        "@tf.function\n",
        "def get_raw_biaxial_stresses(l1, l2, model, W_func):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        l1_t, l2_t = l1, l2; l3_t = 1.0 / (l1_t * l2_t)\n",
        "        tape.watch([l1_t, l2_t, l3_t])\n",
        "        I1, I2, I4, I6 = get_invariants_tf(l1_t, l2_t, l3_t)\n",
        "        W = W_func(I1, I2, I4, I6)\n",
        "    dWdl1, dWdl2, dWdl3 = tape.gradient(W, l1_t), tape.gradient(W, l2_t), tape.gradient(W, l3_t)\n",
        "    del tape\n",
        "    p = l3_t * dWdl3\n",
        "    return dWdl1 - p / l1_t, dWdl2 - p / l2_t\n",
        "@tf.function\n",
        "def get_corrected_biaxial_stresses(l1, l2, model, W_func):\n",
        "    p11_raw, p22_raw = get_raw_biaxial_stresses(l1, l2, model, W_func)\n",
        "    # --- FIX: Explicitly cast constants to float64 ---\n",
        "    p11_offset, p22_offset = get_raw_biaxial_stresses(tf.constant(1.0, dtype=tf.float64), tf.constant(1.0, dtype=tf.float64), model, W_func)\n",
        "    return p11_raw - p11_offset, p22_raw - p22_offset\n",
        "@tf.function\n",
        "def predict_biaxial_p22(lambda2, model, W_func):\n",
        "    l1_min = tf.ones_like(lambda2) * 0.5; l1_max = tf.ones_like(lambda2) * 1.5\n",
        "    for _ in tf.range(50): # Bisection method loop\n",
        "        l1_mid = (l1_min + l1_max) / 2.0\n",
        "        p11_mid, _ = get_corrected_biaxial_stresses(l1_mid, lambda2, model, W_func)\n",
        "        p11_min, _ = get_corrected_biaxial_stresses(l1_min, lambda2, model, W_func)\n",
        "        is_same_sign = tf.sign(p11_mid) == tf.sign(p11_min)\n",
        "        l1_min = tf.where(is_same_sign, l1_mid, l1_min)\n",
        "        l1_max = tf.where(is_same_sign, l1_max, l1_mid)\n",
        "    final_l1 = tf.stop_gradient((l1_min + l1_max) / 2.0)\n",
        "    _, P22_final = get_corrected_biaxial_stresses(final_l1, lambda2, model, W_func)\n",
        "    is_undeformed = tf.abs(lambda2 - 1.0) < 1e-9\n",
        "    return tf.where(is_undeformed, tf.constant(0.0, dtype=tf.float64), P22_final)\n",
        "\n",
        "# --- Experimental Data ---\n",
        "exp_data_raw_uniaxial_cnf = np.array([[1.0000,0],[1.0708,0.3840],[1.2017,0.8987],[1.3125,1.1814],[1.4000,1.4093],[1.5125,1.6456],[1.6017,1.8608],[1.7125,2.1055],[1.8008,2.3122],[1.8883,2.5570],[1.9767,2.7848],[2.0883,3.1519],[2.1992,3.5274],[2.2867,3.8354],[2.3975,4.2532],[2.4383,4.4304],[2.4858,4.5949]])\n",
        "exp_data_raw_biaxial_cnf = np.array([[1.0000,0],[1.3208,1.0506],[1.4017,1.2068],[1.5092,1.3840],[1.5983,1.5401],[1.7017,1.6835],[1.7842,1.7848],[1.8967,1.9662],[1.9792,2.1181],[2.0858,2.2911],[2.1708,2.4599],[2.2783,2.6962],[2.3825,2.9409],[2.4225,3.0549],[2.4867,3.2236]])\n",
        "uniaxial_l1, uniaxial_p11 = [tf.constant(c, dtype=tf.float64) for c in exp_data_raw_uniaxial_cnf.T]\n",
        "biaxial_l2, biaxial_p22 = [tf.constant(c, dtype=tf.float64) for c in exp_data_raw_biaxial_cnf.T]\n",
        "\n",
        "# --- Model Selection ---\n",
        "if MODEL_TYPE == 'exp_power':\n",
        "    model = StrainEnergy_ExpPower_C()\n",
        "elif MODEL_TYPE == 'sinh_cosh_power':\n",
        "    model = StrainEnergy_SinhCoshPower_C()\n",
        "else:\n",
        "    raise ValueError(\"MODEL_TYPE must be 'exp_power' or 'sinh_cosh_power'\")\n",
        "\n",
        "# --- Training Setup ---\n",
        "initial_learning_rate = 0.003\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=2000, decay_rate=0.9, staircase=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "loss_weight_biaxial = tf.constant(10.0, dtype=tf.float64)\n",
        "\n",
        "# --- ENHANCEMENT: Using your more robust hybrid loss function ---\n",
        "@tf.function\n",
        "def train_step(uniaxial_l1, uniaxial_p11, biaxial_l2, biaxial_p22, loss_weight):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Uniaxial Loss (Hybrid)\n",
        "        p11_pred = predict_uniaxial_p11(uniaxial_l1, model, model.call)\n",
        "        relative_error_1 = (p11_pred[1:] - uniaxial_p11[1:]) / (uniaxial_p11[1:] + 1e-9)\n",
        "        loss1_rel = tf.reduce_mean(tf.square(relative_error_1))\n",
        "        loss1_abs = tf.reduce_mean(tf.square(p11_pred - uniaxial_p11))\n",
        "        loss1 = loss1_rel + 0.1 * loss1_abs\n",
        "        # Biaxial Loss (Hybrid)\n",
        "        p22_pred = predict_biaxial_p22(biaxial_l2, model, model.call)\n",
        "        relative_error_2 = (p22_pred[1:] - biaxial_p22[1:]) / (biaxial_p22[1:] + 1e-9)\n",
        "        loss2_rel = tf.reduce_mean(tf.square(relative_error_2))\n",
        "        loss2_abs = tf.reduce_mean(tf.square(p22_pred - biaxial_p22))\n",
        "        loss2 = loss2_rel + 0.1 * loss2_abs\n",
        "        total_loss = loss1 + loss_weight * loss2\n",
        "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "    grads = [tf.clip_by_value(g, -1.0, 1.0) if g is not None else g for g in grads]\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return total_loss, loss1_rel, loss2_rel\n",
        "\n",
        "# --- Training Loop ---\n",
        "epochs = 30000\n",
        "print(f\"--- Training {MODEL_TYPE} model for Calender Tissue ---\");\n",
        "for epoch in range(epochs):\n",
        "    total_loss, loss1, loss2 = train_step(uniaxial_l1, uniaxial_p11, biaxial_l2, biaxial_p22, loss_weight_biaxial)\n",
        "    if epoch % 1000 == 0:\n",
        "        current_lr = lr_schedule(optimizer.iterations)\n",
        "        print(f\"Epoch {epoch:5d}, Loss: {total_loss:.5f} (Uni_rel: {loss1:.5f}, Bi_rel: {loss2:.5f}), LR: {current_lr:.6f}\")\n",
        "print(f\"\\nTraining finished. Final Loss: {total_loss:.5f}\")\n",
        "\n",
        "# --- Contribution Calculation Section ---\n",
        "def get_model_terms(model):\n",
        "    if isinstance(model, StrainEnergy_ExpPower_C):\n",
        "        k1=1.0+tf.exp(model.raw_log_k1); k2=1.5+tf.exp(model.raw_log_k2); k3=1.0+tf.exp(model.raw_log_k3); k4=1.5+tf.exp(model.raw_log_k4)\n",
        "        i1=1.0+tf.exp(model.raw_log_i1); i2=1.0+tf.exp(model.raw_log_i2); i3=1.0+tf.exp(model.raw_log_i3); i4=1.0+tf.exp(model.raw_log_i4)\n",
        "        a1=tf.exp(model.raw_log_a1); a2=tf.exp(model.raw_log_a2); a3=tf.exp(model.raw_log_a3); a4=tf.exp(model.raw_log_a4)\n",
        "        a3_prime=tf.exp(model.raw_log_a3_prime); a4_prime=tf.exp(model.raw_log_a4_prime)\n",
        "        k9=1.0+tf.exp(model.raw_log_k9); k10=1.5+tf.exp(model.raw_log_k10); k11=1.0+tf.exp(model.raw_log_k11); k12=1.5+tf.exp(model.raw_log_k12)\n",
        "        b1=tf.exp(model.raw_log_b1); b2=tf.exp(model.raw_log_b2); b3=tf.exp(model.raw_log_b3); b4=tf.exp(model.raw_log_b4)\n",
        "        b3_prime=tf.exp(model.raw_log_b3_prime); b4_prime=tf.exp(model.raw_log_b4_prime)\n",
        "        return {\n",
        "            \"Pow(I1)\": lambda I1,I2,I4,I6: model._term_power_law(I1,k1,i1,a1,model.three),\n",
        "            \"Pow(I2)\": lambda I1,I2,I4,I6: model._term_power_law(I2,k2,i2,a2,model.three),\n",
        "            \"Exp(I1)\": lambda I1,I2,I4,I6: model._term_exponential(I1,k3,i3,a3_prime,a3,model.three),\n",
        "            \"Exp(I2)\": lambda I1,I2,I4,I6: model._term_exponential(I2,k4,i4,a4_prime,a4,model.three),\n",
        "            \"Pow(I4)\": lambda I1,I2,I4,I6: model._term_identity_scaled(I4,k9,b1,model.one),\n",
        "            \"Pow(I6)\": lambda I1,I2,I4,I6: model._term_identity_scaled(I6,k10,b2,model.one),\n",
        "            \"Exp(I4)\": lambda I1,I2,I4,I6: model._term_exponential_no_i(I4,k11,b3_prime,b3,model.one),\n",
        "            \"Exp(I6)\": lambda I1,I2,I4,I6: model._term_exponential_no_i(I6,k12,b4_prime,b4,model.one),\n",
        "        }\n",
        "    elif isinstance(model, StrainEnergy_SinhCoshPower_C):\n",
        "        k1=1.0+tf.exp(model.raw_log_k1);k2=1.5+tf.exp(model.raw_log_k2);k5=1.0+tf.exp(model.raw_log_k5);k6=1.5+tf.exp(model.raw_log_k6);k7=1.0+tf.exp(model.raw_log_k7);k8=1.5+tf.exp(model.raw_log_k8);k13=1.0+tf.exp(model.raw_log_k13);k14=1.5+tf.exp(model.raw_log_k14);k15=1.0+tf.exp(model.raw_log_k15);k16=1.5+tf.exp(model.raw_log_k16)\n",
        "        i1=1.0+tf.exp(model.raw_log_i1);i2=1.0+tf.exp(model.raw_log_i2);i5=1.0+tf.exp(model.raw_log_i5);i6=1.0+tf.exp(model.raw_log_i6);i7=1.0+tf.exp(model.raw_log_i7);i8=1.0+tf.exp(model.raw_log_i8)\n",
        "        a1=tf.exp(model.raw_log_a1);a2=tf.exp(model.raw_log_a2);a5=tf.exp(model.raw_log_a5);a6=tf.exp(model.raw_log_a6);a7=tf.exp(model.raw_log_a7);a8=tf.exp(model.raw_log_a8)\n",
        "        a5_prime=tf.exp(model.raw_log_a5_prime);a6_prime=tf.exp(model.raw_log_a6_prime);a7_prime=tf.exp(model.raw_log_a7_prime);a8_prime=tf.exp(model.raw_log_a8_prime)\n",
        "        b5=tf.exp(model.raw_log_b5);b6=tf.exp(model.raw_log_b6);b7=tf.exp(model.raw_log_b7);b8=tf.exp(model.raw_log_b8)\n",
        "        b5_prime=tf.exp(model.raw_log_b5_prime);b6_prime=tf.exp(model.raw_log_b6_prime);b7_prime=tf.exp(model.raw_log_b7_prime);b8_prime=tf.exp(model.raw_log_b8_prime)\n",
        "        return {\n",
        "            \"Pow(I1)\":  lambda I1,I2,I4,I6: model._term_power_law(I1,k1,i1,a1,model.three),\n",
        "            \"Pow(I2)\":  lambda I1,I2,I4,I6: model._term_power_law(I2,k2,i2,a2,model.three),\n",
        "            \"Cosh(I1)\": lambda I1,I2,I4,I6: model._term_cosh_minus_one_with_i(I1,k5,i5,a5_prime,a5,model.three),\n",
        "            \"Cosh(I2)\": lambda I1,I2,I4,I6: model._term_cosh_minus_one_with_i(I2,k6,i6,a6_prime,a6,model.three),\n",
        "            \"Sinh(I1)\": lambda I1,I2,I4,I6: model._term_sinh_with_i(I1,k7,i7,a7_prime,a7,model.three),\n",
        "            \"Sinh(I2)\": lambda I1,I2,I4,I6: model._term_sinh_with_i(I2,k8,i8,a8_prime,a8,model.three),\n",
        "            \"Cosh(I4)\": lambda I1,I2,I4,I6: model._term_cosh_minus_one(I4,k13,b5_prime,b5,model.one),\n",
        "            \"Cosh(I6)\": lambda I1,I2,I4,I6: model._term_cosh_minus_one(I6,k14,b6_prime,b6,model.one),\n",
        "            \"Sinh(I4)\": lambda I1,I2,I4,I6: model._term_sinh(I4,k15,b7_prime,b7,model.one),\n",
        "            \"Sinh(I6)\": lambda I1,I2,I4,I6: model._term_sinh(I6,k16,b8_prime,b8,model.one),\n",
        "        }\n",
        "\n",
        "def calculate_stress_contributions(lambda_values, model, task_type):\n",
        "    terms = get_model_terms(model)\n",
        "    contributions = {}\n",
        "    print(f\"\\nCalculating contributions for {task_type}...\")\n",
        "    for name, W_func in terms.items():\n",
        "        if task_type == 'uniaxial':\n",
        "            stress_contribution = predict_uniaxial_p11(lambda_values, model, W_func)\n",
        "        else: # biaxial\n",
        "            stress_contribution = predict_biaxial_p22(lambda_values, model, W_func)\n",
        "        contributions[name] = stress_contribution.numpy()\n",
        "        print(f\"  - Calculated contribution for term: {name}\")\n",
        "    return contributions\n",
        "\n",
        "# --- Run Calculation and Save ---\n",
        "lambda1_plot_tf = tf.constant(np.linspace(1.0, 2.5, 200), dtype=tf.float64)\n",
        "lambda2_plot_tf = tf.constant(np.linspace(1.0, 2.5, 200), dtype=tf.float64)\n",
        "p11_contributions = calculate_stress_contributions(lambda1_plot_tf, model, 'uniaxial')\n",
        "p22_contributions = calculate_stress_contributions(lambda2_plot_tf, model, 'biaxial')\n",
        "\n",
        "df_contrib = pd.DataFrame()\n",
        "df_contrib['lambda1'] = lambda1_plot_tf.numpy()\n",
        "for name, values in p11_contributions.items():\n",
        "    df_contrib[f'P11_{name}'] = values\n",
        "df_contrib['lambda2'] = lambda2_plot_tf.numpy()\n",
        "for name, values in p22_contributions.items():\n",
        "    df_contrib[f'P22_{name}'] = values\n",
        "\n",
        "output_filename = f'calender_contributions_{MODEL_TYPE}.csv'\n",
        "df_contrib.to_csv(output_filename, index=False)\n",
        "print(f\"\\nSuccessfully saved stress contributions to '{output_filename}'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# ===== NEW SECTION: EXTRACT AND PRINT FINAL LEARNED PARAMETERS ==============\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"      Final Learned Model Parameters (Raw Log Form)\")\n",
        "print(\"=\"*60)\n",
        "# This prints the raw values that the optimizer sees.\n",
        "for v in model.trainable_variables: # Assuming your trained model is named 'model'\n",
        "    print(f\"{v.name:20s}: {v.numpy():.8f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"   Transformed Model Parameters (Physical Interpretable Values)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# --- Extract and Transform Parameters specific to the Exp-Power model ---\n",
        "k1 = 1.0 + tf.exp(model.raw_log_k1).numpy()\n",
        "k2 = 1.5 + tf.exp(model.raw_log_k2).numpy()\n",
        "k3 = 1.0 + tf.exp(model.raw_log_k3).numpy()\n",
        "k4 = 1.5 + tf.exp(model.raw_log_k4).numpy()\n",
        "i1 = 1.0 + tf.exp(model.raw_log_i1).numpy()\n",
        "i2 = 1.0 + tf.exp(model.raw_log_i2).numpy()\n",
        "i3 = 1.0 + tf.exp(model.raw_log_i3).numpy()\n",
        "i4 = 1.0 + tf.exp(model.raw_log_i4).numpy()\n",
        "a1 = tf.exp(model.raw_log_a1).numpy()\n",
        "a2 = tf.exp(model.raw_log_a2).numpy()\n",
        "a3 = tf.exp(model.raw_log_a3).numpy()\n",
        "a4 = tf.exp(model.raw_log_a4).numpy()\n",
        "a3_prime = tf.exp(model.raw_log_a3_prime).numpy()\n",
        "a4_prime = tf.exp(model.raw_log_a4_prime).numpy()\n",
        "\n",
        "k9 = 1.0 + tf.exp(model.raw_log_k9).numpy()\n",
        "k10 = 1.5 + tf.exp(model.raw_log_k10).numpy()\n",
        "k11 = 1.0 + tf.exp(model.raw_log_k11).numpy()\n",
        "k12 = 1.5 + tf.exp(model.raw_log_k12).numpy()\n",
        "b1 = tf.exp(model.raw_log_b1).numpy()\n",
        "b2 = tf.exp(model.raw_log_b2).numpy()\n",
        "b3 = tf.exp(model.raw_log_b3).numpy()\n",
        "b4 = tf.exp(model.raw_log_b4).numpy()\n",
        "b3_prime = tf.exp(model.raw_log_b3_prime).numpy()\n",
        "b4_prime = tf.exp(model.raw_log_b4_prime).numpy()\n",
        "\n",
        "\n",
        "# --- Print in a clean, organized table format ---\n",
        "print(f\"{'Parameter':<12} | {'Value':<15} | {'Parameter':<12} | {'Value'}\")\n",
        "print(\"-\" * 55)\n",
        "print(f\"{'k1':<12} | {k1:<15.5f} | {'i1':<12} | {i1:<15.5f}\")\n",
        "print(f\"{'k2':<12} | {k2:<15.5f} | {'i2':<12} | {i2:<15.5f}\")\n",
        "print(f\"{'k3':<12} | {k3:<15.5f} | {'i3':<12} | {i3:<15.5f}\")\n",
        "print(f\"{'k4':<12} | {k4:<15.5f} | {'i4':<12} | {i4:<15.5f}\")\n",
        "print(f\"{'k9':<12} | {k9:<15.5f} |\")\n",
        "print(f\"{'k10':<12} | {k10:<15.5f} |\")\n",
        "print(f\"{'k11':<12} | {k11:<15.5f} |\")\n",
        "print(f\"{'k12':<12} | {k12:<15.5f} |\")\n",
        "print(\"-\" * 55)\n",
        "print(f\"{'a1':<12} | {a1:<15.5f} | {'b1':<12} | {b1:<15.5f}\")\n",
        "print(f\"{'a2':<12} | {a2:<15.5f} | {'b2':<12} | {b2:<15.5f}\")\n",
        "print(f\"{'a3':<12} | {a3:<15.5f} | {'b3':<12} | {b3:<15.5f}\")\n",
        "print(f\"{'a4':<12} | {a4:<15.5f} | {'b4':<12} | {b4:<15.5f}\")\n",
        "print(f\"{'a3_prime':<12} | {a3_prime:<15.5f} | {'b3_prime':<12} | {b3_prime:<15.5f}\")\n",
        "print(f\"{'a4_prime':<12} | {a4_prime:<15.5f} | {'b4_prime':<12} | {b4_prime:<15.5f}\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtk2d83fUOAR",
        "outputId": "5bcc4dfa-035c-488c-9db3-9d1eeaa56caf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "      Final Learned Model Parameters (Raw Log Form)\n",
            "============================================================\n",
            "raw_log_k1          : -0.76134237\n",
            "raw_log_k2          : -6.61749669\n",
            "raw_log_i1          : -1.91737236\n",
            "raw_log_i2          : -9.68820560\n",
            "raw_log_a1          : -3.59958873\n",
            "raw_log_a2          : -1.68899257\n",
            "raw_log_k3          : -1.32550620\n",
            "raw_log_k4          : -3.22068638\n",
            "raw_log_i3          : -0.79624336\n",
            "raw_log_i4          : -4.19095797\n",
            "raw_log_a3          : -5.97637289\n",
            "raw_log_a4          : -5.34329170\n",
            "raw_log_a3_prime    : -3.64781758\n",
            "raw_log_a4_prime    : -5.29524916\n",
            "raw_log_k9          : 0.15210526\n",
            "raw_log_k10         : -0.14570058\n",
            "raw_log_b1          : -3.68273651\n",
            "raw_log_b2          : -4.36455860\n",
            "raw_log_k11         : -1.53415257\n",
            "raw_log_k12         : 1.15807349\n",
            "raw_log_b3          : -2.01139069\n",
            "raw_log_b4          : -2.65077483\n",
            "raw_log_b3_prime    : -2.60560234\n",
            "raw_log_b4_prime    : -0.71744247\n",
            "\n",
            "============================================================\n",
            "   Transformed Model Parameters (Physical Interpretable Values)\n",
            "============================================================\n",
            "Parameter    | Value           | Parameter    | Value\n",
            "-------------------------------------------------------\n",
            "k1           | 1.46704         | i1           | 1.14699        \n",
            "k2           | 1.50134         | i2           | 1.00006        \n",
            "k3           | 1.26567         | i3           | 1.45102        \n",
            "k4           | 1.53993         | i4           | 1.01513        \n",
            "k9           | 2.16428         |\n",
            "k10          | 2.36442         |\n",
            "k11          | 1.21564         |\n",
            "k12          | 4.68379         |\n",
            "-------------------------------------------------------\n",
            "a1           | 0.02733         | b1           | 0.02515        \n",
            "a2           | 0.18471         | b2           | 0.01272        \n",
            "a3           | 0.00254         | b3           | 0.13380        \n",
            "a4           | 0.00478         | b4           | 0.07060        \n",
            "a3_prime     | 0.02605         | b3_prime     | 0.07386        \n",
            "a4_prime     | 0.00502         | b4_prime     | 0.48800        \n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V0O9Moddkmox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}