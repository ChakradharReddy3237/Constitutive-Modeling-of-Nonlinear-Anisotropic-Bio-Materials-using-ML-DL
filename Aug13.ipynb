{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc5be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow -q\n",
    "!pip install -U \"seaborn\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57096464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377484b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: /GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755090042.372533   14578 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1238 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# --- Setup, Model Definition, Helper/Physics Functions ---\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "DEVICE = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "ARG_CLIP_MIN = tf.constant(-10.0, dtype=tf.float64)\n",
    "ARG_CLIP_MAX = tf.constant(10.0, dtype=tf.float64)\n",
    "\n",
    "@tf.function\n",
    "def smooth_relu(x, beta=20.0):\n",
    "    \"\"\"A smooth, differentiable approximation of the ReLU function.\"\"\"\n",
    "    return tf.nn.softplus(beta * x) / beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97534bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrainEnergyANN_Layered_TF(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"StrainEnergyModel\")\n",
    "        # --- MODIFICATION: REMOVE b1 FROM TRAINABLE PARAMETERS ---\n",
    "        # b1 will be calculated from the other parameters to enforce the constraint.\n",
    "        # self.raw_log_b1 = self.add_weight(...) # This line is removed.\n",
    "\n",
    "        # k & i params (exponents) - All other parameters remain trainable\n",
    "        self.raw_log_k1=self.add_weight(name=\"raw_log_k1\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k2=self.add_weight(name=\"raw_log_k2\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k3=self.add_weight(name=\"raw_log_k3\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k4=self.add_weight(name=\"raw_log_k4\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k5=self.add_weight(name=\"raw_log_k5\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k6=self.add_weight(name=\"raw_log_k6\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k7=self.add_weight(name=\"raw_log_k7\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k8=self.add_weight(name=\"raw_log_k8\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k9=self.add_weight(name=\"raw_log_k9\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k10=self.add_weight(name=\"raw_log_k10\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k11=self.add_weight(name=\"raw_log_k11\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k12=self.add_weight(name=\"raw_log_k12\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k13=self.add_weight(name=\"raw_log_k13\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k14=self.add_weight(name=\"raw_log_k14\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k15=self.add_weight(name=\"raw_log_k15\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_k16=self.add_weight(name=\"raw_log_k16\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_i1=self.add_weight(name=\"raw_log_i1\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_i2=self.add_weight(name=\"raw_log_i2\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_i3=self.add_weight(name=\"raw_log_i3\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_i4=self.add_weight(name=\"raw_log_i4\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_i5=self.add_weight(name=\"raw_log_i5\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_i6=self.add_weight(name=\"raw_log_i6\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_i7=self.add_weight(name=\"raw_log_i7\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_i8=self.add_weight(name=\"raw_log_i8\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.01),trainable=True)\n",
    "        self.raw_log_a1=self.add_weight(name=\"raw_log_a1\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_a2=self.add_weight(name=\"raw_log_a2\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_a3=self.add_weight(name=\"raw_log_a3\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_a4=self.add_weight(name=\"raw_log_a4\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_a5=self.add_weight(name=\"raw_log_a5\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_a6=self.add_weight(name=\"raw_log_a6\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_a7=self.add_weight(name=\"raw_log_a7\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_a8=self.add_weight(name=\"raw_log_a8\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_b2=self.add_weight(name=\"raw_log_b2\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.01)),trainable=True)\n",
    "        self.raw_log_b3=self.add_weight(name=\"raw_log_b3\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_b4=self.add_weight(name=\"raw_log_b4\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_b5=self.add_weight(name=\"raw_log_b5\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_b6=self.add_weight(name=\"raw_log_b6\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_b7=self.add_weight(name=\"raw_log_b7\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_b8=self.add_weight(name=\"raw_log_b8\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.1)),trainable=True)\n",
    "        self.raw_log_a3_prime=self.add_weight(name=\"raw_log_a3_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
    "        self.raw_log_a4_prime=self.add_weight(name=\"raw_log_a4_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
    "        self.raw_log_a5_prime=self.add_weight(name=\"raw_log_a5_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
    "        self.raw_log_a6_prime=self.add_weight(name=\"raw_log_a6_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
    "        self.raw_log_a7_prime=self.add_weight(name=\"raw_log_a7_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
    "        self.raw_log_a8_prime=self.add_weight(name=\"raw_log_a8_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
    "        self.raw_log_b3_prime=self.add_weight(name=\"raw_log_b3_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
    "        self.raw_log_b4_prime=self.add_weight(name=\"raw_log_b4_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
    "        self.raw_log_b5_prime=self.add_weight(name=\"raw_log_b5_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
    "        self.raw_log_b6_prime=self.add_weight(name=\"raw_log_b6_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
    "        self.raw_log_b7_prime=self.add_weight(name=\"raw_log_b7_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
    "        self.raw_log_b8_prime=self.add_weight(name=\"raw_log_b8_prime\",shape=(),dtype=tf.float64,initializer=tf.keras.initializers.Constant(tf.math.log(0.2)),trainable=True)\n",
    "\n",
    "        self.three=tf.constant(3.0,dtype=tf.float64)\n",
    "        self.one=tf.constant(1.0,dtype=tf.float64)\n",
    "        self.pow_base_epsilon=tf.constant(1e-8,dtype=tf.float64)\n",
    "\n",
    "    def _term_power_law(self, I, k, i, c, ref_val):\n",
    "        arg = smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k))\n",
    "        return c * tf.pow(arg + self.pow_base_epsilon, i)\n",
    "    def _term_exponential(self, I, k, i, ic, oc, ref_val):\n",
    "        arg = smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k))\n",
    "        return oc * (tf.exp(tf.clip_by_value(ic * tf.pow(arg + self.pow_base_epsilon, i), ARG_CLIP_MIN, ARG_CLIP_MAX)) - 1.0)\n",
    "    def _term_cosh_minus_one_with_i(self, I, k, i, ic, oc, ref_val):\n",
    "        arg = smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k))\n",
    "        return oc * (tf.cosh(tf.clip_by_value(ic * tf.pow(arg + self.pow_base_epsilon, i), ARG_CLIP_MIN, ARG_CLIP_MAX)) - 1.0)\n",
    "    def _term_sinh_with_i(self, I, k, i, ic, oc, ref_val):\n",
    "        arg = smooth_relu(tf.pow(I, k) - tf.pow(ref_val, k))\n",
    "        return oc * tf.sinh(tf.clip_by_value(ic * tf.pow(arg + self.pow_base_epsilon, i), ARG_CLIP_MIN, ARG_CLIP_MAX))\n",
    "    def _term_identity_scaled(self, I, k, c, ref_val):\n",
    "        return c * (tf.pow(I, k) - tf.pow(ref_val, k))\n",
    "    def _term_exponential_no_i(self, I, k, ic, oc, ref_val):\n",
    "        return oc * (tf.exp(tf.clip_by_value(ic * (tf.pow(I, k) - tf.pow(ref_val, k)), ARG_CLIP_MIN, ARG_CLIP_MAX)) - 1.0)\n",
    "    def _term_cosh_minus_one(self, I, k, ic, oc, ref_val):\n",
    "        return oc * (tf.cosh(tf.clip_by_value(ic * (tf.pow(I, k) - tf.pow(ref_val, k)), ARG_CLIP_MIN, ARG_CLIP_MAX)) - 1.0)\n",
    "    def _term_sinh(self, I, k, ic, oc, ref_val):\n",
    "        return oc * tf.sinh(tf.clip_by_value(ic * (tf.pow(I, k) - tf.pow(ref_val, k)), ARG_CLIP_MIN, ARG_CLIP_MAX))\n",
    "\n",
    "    def call(self, I1, I2, I4, I6):\n",
    "        k1=1.0+tf.exp(self.raw_log_k1);k2=1.5+tf.exp(self.raw_log_k2);k3=1.0+tf.exp(self.raw_log_k3);k4=1.5+tf.exp(self.raw_log_k4);k5=1.0+tf.exp(self.raw_log_k5);k6=1.5+tf.exp(self.raw_log_k6);k7=1.0+tf.exp(self.raw_log_k7);k8=1.5+tf.exp(self.raw_log_k8);k9=1.0+tf.exp(self.raw_log_k9);k10=1.5+tf.exp(self.raw_log_k10);k11=1.0+tf.exp(self.raw_log_k11);k12=1.5+tf.exp(self.raw_log_k12);k13=1.0+tf.exp(self.raw_log_k13);k14=1.5+tf.exp(self.raw_log_k14);k15=1.0+tf.exp(self.raw_log_k15);k16=1.5+tf.exp(self.raw_log_k16)\n",
    "        i1=1.0+tf.exp(self.raw_log_i1);i2=1.0+tf.exp(self.raw_log_i2);i3=1.0+tf.exp(self.raw_log_i3);i4=1.0+tf.exp(self.raw_log_i4);i5=1.0+tf.exp(self.raw_log_i5);i6=1.0+tf.exp(self.raw_log_i6);i7=1.0+tf.exp(self.raw_log_i7);i8=1.0+tf.exp(self.raw_log_i8)\n",
    "        a1=tf.exp(self.raw_log_a1);a2=tf.exp(self.raw_log_a2);a3=tf.exp(self.raw_log_a3);a4=tf.exp(self.raw_log_a4);a5=tf.exp(self.raw_log_a5);a6=tf.exp(self.raw_log_a6);a7=tf.exp(self.raw_log_a7);a8=tf.exp(self.raw_log_a8)\n",
    "        a3_prime=tf.exp(self.raw_log_a3_prime);a4_prime=tf.exp(self.raw_log_a4_prime);a5_prime=tf.exp(self.raw_log_a5_prime);a6_prime=tf.exp(self.raw_log_a6_prime);a7_prime=tf.exp(self.raw_log_a7_prime);a8_prime=tf.exp(self.raw_log_a8_prime)\n",
    "\n",
    "        b2=tf.exp(self.raw_log_b2);b3=tf.exp(self.raw_log_b3);b4=tf.exp(self.raw_log_b4);b5=tf.exp(self.raw_log_b5);b6=tf.exp(self.raw_log_b6);b7=tf.exp(self.raw_log_b7);b8=tf.exp(self.raw_log_b8)\n",
    "        b3_prime=tf.exp(self.raw_log_b3_prime);b4_prime=tf.exp(self.raw_log_b4_prime);b5_prime=tf.exp(self.raw_log_b5_prime);b6_prime=tf.exp(self.raw_log_b6_prime);b7_prime=tf.exp(self.raw_log_b7_prime);b8_prime=tf.exp(self.raw_log_b8_prime)\n",
    "\n",
    "        # --- MODIFICATION: ENFORCE CONSTRAINT VIA REPARAMETERIZATION ---\n",
    "        rhs = (b2 * k10) + (b4 * b4_prime * k12) + (b8 * b8_prime * k16)\n",
    "        lhs_sub = (b3 * b3_prime * k11) + (b7 * b7_prime * k15)\n",
    "        # k9 is guaranteed to be > 1.0, so division is safe. Add a small epsilon for absolute stability.\n",
    "        b1 = (rhs - lhs_sub) / (k9 + 1e-8)\n",
    "\n",
    "        W = tf.zeros_like(I1,dtype=tf.float64)\n",
    "        W += self._term_power_law(I1,k1,i1,a1,self.three); W += self._term_power_law(I2,k2,i2,a2,self.three)\n",
    "        W += self._term_exponential(I1,k3,i3,a3_prime,a3,self.three); W += self._term_exponential(I2,k4,i4,a4_prime,a4,self.three)\n",
    "        W += self._term_cosh_minus_one_with_i(I1,k5,i5,a5_prime,a5,self.three); W += self._term_cosh_minus_one_with_i(I2,k6,i6,a6_prime,a6,self.three)\n",
    "        W += self._term_sinh_with_i(I1,k7,i7,a7_prime,a7,self.three); W += self._term_sinh_with_i(I2,k8,i8,a8_prime,a8,self.three)\n",
    "        # Use the computed b1 here\n",
    "        W += self._term_identity_scaled(I4,k9,b1,self.one); W += self._term_identity_scaled(I6,k10,b2,self.one)\n",
    "        W += self._term_exponential_no_i(I4,k11,b3_prime,b3,self.one); W += self._term_exponential_no_i(I6,k12,b4_prime,b4,self.one)\n",
    "        W += self._term_cosh_minus_one(I4,k13,b5_prime,b5,self.one); W += self._term_cosh_minus_one(I6,k14,b6_prime,b6,self.one)\n",
    "        W += self._term_sinh(I4,k15,b7_prime,b7,self.one); W += self._term_sinh(I6,k16,b8_prime,b8,self.one)\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d38bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_invariants_tf(lambda1, lambda2, lambda3):\n",
    "    min_lambda_val = tf.constant(1e-6, dtype=tf.float64)\n",
    "    lambda1 = tf.maximum(lambda1, min_lambda_val)\n",
    "    lambda2 = tf.maximum(lambda2, min_lambda_val)\n",
    "    lambda3 = tf.maximum(lambda3, min_lambda_val)\n",
    "    l1s = tf.pow(lambda1, 2.0); l2s = tf.pow(lambda2, 2.0); l3s = tf.pow(lambda3, 2.0)\n",
    "    I1 = l1s + l2s + l3s\n",
    "    I2 = tf.pow(lambda1 * lambda2, 2.0) + tf.pow(lambda2 * lambda3, 2.0) + tf.pow(lambda3 * lambda1, 2.0)\n",
    "    I4 = l1s\n",
    "    # I6 has an alternative definition depending on fiber orientation. Assuming a_0 = [1,0,0]\n",
    "    I6 = 1/l1s\n",
    "    return I1, I2, I4, I6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d7ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def predict_uniaxial_p11(lambda1, model):\n",
    "    l1 = lambda1\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        l1_t = tf.identity(l1)\n",
    "        # Incompressible material: lambda1 * lambda2 * lambda3 = 1\n",
    "        l2_t = tf.pow(l1_t, -0.5)\n",
    "        l3_t = tf.pow(l1_t, -0.5)\n",
    "        tape.watch([l1_t, l2_t, l3_t])\n",
    "        I1, I2, I4, I6 = get_invariants_tf(l1_t, l2_t, l3_t)\n",
    "        W = model(I1, I2, I4, I6)\n",
    "\n",
    "    dWdl1 = tape.gradient(W, l1_t)\n",
    "    dWdl2 = tape.gradient(W, l2_t)\n",
    "    dWdl3 = tape.gradient(W, l3_t)\n",
    "    del tape\n",
    "\n",
    "    # Cauchy stress T = (2/J) * [ (dW/dI1 + I1*dW/dI2)*B - (dW/dI2)*B^2 ... ] * F^T\n",
    "    # For uniaxial, P11 = 2 * (l1 - 1/l1^2) * (dW/dI1 + 1/l1 * dW/dI2) + 2 * (l1-1/l1^2)*dW/dI4\n",
    "    # A more direct approach using principal stresses for incompressible materials:\n",
    "    # P_ii = dW/d_lambda_i - p / lambda_i\n",
    "    # For uniaxial test, P22 = P33 = 0. So p = lambda2 * dW/d_lambda2 = lambda3 * dW/d_lambda3\n",
    "    p = l2_t * dWdl2\n",
    "    P11 = dWdl1 - p / l1_t\n",
    "    return P11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9fe911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_biaxial_stresses(l1, l2, model):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        l1_t = tf.identity(l1)\n",
    "        l2_t = tf.identity(l2)\n",
    "        # Incompressible material: lambda1 * lambda2 * lambda3 = 1\n",
    "        l3_t = 1.0 / (l1_t * l2_t)\n",
    "        tape.watch([l1_t, l2_t, l3_t])\n",
    "        I1, I2, I4, I6 = get_invariants_tf(l1_t, l2_t, l3_t)\n",
    "        W = model(I1, I2, I4, I6)\n",
    "\n",
    "    dWdl1 = tape.gradient(W, l1_t)\n",
    "    dWdl2 = tape.gradient(W, l2_t)\n",
    "    dWdl3 = tape.gradient(W, l3_t)\n",
    "    del tape\n",
    "\n",
    "    # For planar tension, P33 = 0. So p = lambda3 * dW/d_lambda3\n",
    "    p = l3_t * dWdl3\n",
    "    P11 = dWdl1 - p / l1_t\n",
    "    P22 = dWdl2 - p / l2_t\n",
    "    return P11, P22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "980e1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def predict_biaxial_p22(lambda2, model):\n",
    "    l1_min = tf.ones_like(lambda2, dtype=tf.float64) * 0.5\n",
    "    l1_max = tf.ones_like(lambda2, dtype=tf.float64) * 1.5\n",
    "\n",
    "    # Use bisection method to find lambda1 such that P11=0\n",
    "    for _ in range(25): # Increased iterations for better precision\n",
    "        l1_mid = (l1_min + l1_max) / 2.0\n",
    "        p11_mid, _ = get_biaxial_stresses(l1_mid, lambda2, model)\n",
    "        p11_min, _ = get_biaxial_stresses(l1_min, lambda2, model)\n",
    "\n",
    "        is_same_sign = tf.sign(p11_mid) == tf.sign(p11_min)\n",
    "        l1_min = tf.where(is_same_sign, l1_mid, l1_min)\n",
    "        l1_max = tf.where(is_same_sign, l1_max, l1_mid)\n",
    "\n",
    "    final_l1 = tf.stop_gradient((l1_min + l1_max) / 2.0)\n",
    "    _, P22_final = get_biaxial_stresses(final_l1, lambda2, model)\n",
    "    return P22_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44c1bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experimental Data ---\n",
    "exp_data_raw_uniaxial_cnf = np.array([\n",
    "    [1.0000, 0], [1.0708, 0.3840], [1.2017, 0.8987], [1.3125, 1.1814], [1.4000, 1.4093],\n",
    "    [1.5125, 1.6456], [1.6017, 1.8608], [1.7125, 2.1055], [1.8008, 2.3122], [1.8883, 2.5570],\n",
    "    [1.9767, 2.7848], [2.0883, 3.1519], [2.1992, 3.5274], [2.2867, 3.8354], [2.3975, 4.2532],\n",
    "    [2.4383, 4.4304], [2.4858, 4.5949]\n",
    "])\n",
    "exp_data_raw_biaxial_cnf = np.array([\n",
    "    [1.0000, 0], [1.3208, 1.0506], [1.4017, 1.2068], [1.5092, 1.3840], [1.5983, 1.5401],\n",
    "    [1.7017, 1.6835], [1.7842, 1.7848], [1.8967, 1.9662], [1.9792, 2.1181], [2.0858, 2.2911],\n",
    "    [2.1708, 2.4599], [2.2783, 2.6962], [2.3825, 2.9409], [2.4225, 3.0549], [2.4867, 3.2236]\n",
    "])\n",
    "uniaxial_l1, uniaxial_p11 = [tf.constant(c, dtype=tf.float64) for c in exp_data_raw_uniaxial_cnf.T]\n",
    "biaxial_l2, biaxial_p22 = [tf.constant(c, dtype=tf.float64) for c in exp_data_raw_biaxial_cnf.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b669511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training with CONSTRAINED & IMPROVED LOSS FUNCTION ---\n",
    "\n",
    "model = StrainEnergyANN_Layered_TF()\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=2000, decay_rate=0.9, staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "loss_weight_biaxial = tf.constant(1.0, dtype=tf.float64)\n",
    "# --- MODIFICATION: ADD WEIGHT FOR THE NEW ZERO-STRESS PENALTY ---\n",
    "zero_stress_weight = tf.constant(100.0, dtype=tf.float64)\n",
    "\n",
    "print(f\"Biaxial Loss Weight: {loss_weight_biaxial.numpy():.2f}\")\n",
    "print(f\"Zero-Stress Penalty Weight: {zero_stress_weight.numpy():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7560f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization ---\n",
    "\n",
    "# Generate predictions for plotting\n",
    "l1_plot = np.linspace(1.0, exp_data_raw_uniaxial_cnf[:, 0].max(), 100)\n",
    "p11_pred_plot = predict_uniaxial_p11(tf.constant(l1_plot, dtype=tf.float64), model).numpy()\n",
    "\n",
    "l2_plot = np.linspace(1.0, exp_data_raw_biaxial_cnf[:, 0].max(), 100)\n",
    "p22_pred_plot = predict_biaxial_p22(tf.constant(l2_plot, dtype=tf.float64), model).numpy()\n",
    "\n",
    "# Create plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "# Uniaxial Plot\n",
    "axes[0].plot(exp_data_raw_uniaxial_cnf[:, 0], exp_data_raw_uniaxial_cnf[:, 1], 'ro', label='Experimental Data')\n",
    "axes[0].plot(l1_plot, p11_pred_plot, 'b-', lw=2, label='Model Prediction (P11)')\n",
    "axes[0].set_xlabel('Stretch $\\\\lambda_1$', fontsize=12)\n",
    "axes[0].set_ylabel('Nominal Stress $P_{11}$ (MPa)', fontsize=12)\n",
    "axes[0].set_title('Task 1: Uniaxial Tension Fit', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(bottom=0)\n",
    "axes[0].set_xlim(left=1)\n",
    "\n",
    "# Biaxial Plot\n",
    "axes[1].plot(exp_data_raw_biaxial_cnf[:, 0], exp_data_raw_biaxial_cnf[:, 1], 'go', label='Experimental Data')\n",
    "axes[1].plot(l2_plot, p22_pred_plot, 'b-', lw=2, label='Model Prediction (P22)')\n",
    "axes[1].set_xlabel('Stretch $\\\\lambda_2$', fontsize=12)\n",
    "axes[1].set_ylabel('Nominal Stress $P_{22}$ (MPa)', fontsize=12)\n",
    "axes[1].set_title('Task 2: Biaxial (Planar) Tension Fit', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(bottom=0)\n",
    "axes[1].set_xlim(left=1)\n",
    "\n",
    "# Loss History Plot\n",
    "ax2 = axes[2]\n",
    "ax2.plot(loss_history, label='Total Loss', color='darkblue')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Total Loss (MSRE + Penalty)', fontsize=12, color='darkblue')\n",
    "ax2.set_title('Training History', fontsize=14)\n",
    "ax2.set_yscale('log')\n",
    "ax2.tick_params(axis='y', labelcolor='darkblue')\n",
    "ax2.grid(True, which=\"both\", ls=\"--\")\n",
    "\n",
    "# Learning Rate Plot on the same axes\n",
    "ax3 = ax2.twinx()\n",
    "ax3.plot(learning_rate_history, label='Learning Rate', color='red', linestyle='--')\n",
    "ax3.set_ylabel('Learning Rate', fontsize=12, color='red')\n",
    "ax3.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Final Check ---\n",
    "# Final check to confirm stresses are very close to zero at lambda=1\n",
    "p11_final_check = predict_uniaxial_p11(tf.constant([1.0], dtype=tf.float64), model)\n",
    "p22_final_check = predict_biaxial_p22(tf.constant([1.0], dtype=tf.float64), model)\n",
    "print(\"\\nFinal check:\")\n",
    "print(f\"P11 at lambda1=1: {p11_final_check.numpy()[0]:.6e}\")\n",
    "print(f\"P22 at lambda2=1: {p22_final_check.numpy()[0]:.6e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chicken (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
